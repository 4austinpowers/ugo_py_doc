
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A Python documentation website.">
      
      
      
      
      <link rel="icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Feature Selection - ugo_py_doc</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffc105">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Ubuntu";--md-code-font-family:"Ubuntu Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="amber" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#a-feature-selection-case" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ugo_py_doc" class="md-header__button md-logo" aria-label="ugo_py_doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12 16 7.36-5.73L21 9l-9-7-9 7 1.63 1.27M12 18.54l-7.38-5.73L3 14.07l9 7 9-7-1.63-1.27L12 18.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ugo_py_doc
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Feature Selection
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/ugoproto/ugo_py_doc.git/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ugo_py_doc
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ugo_py_doc" class="md-nav__button md-logo" aria-label="ugo_py_doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12 16 7.36-5.73L21 9l-9-7-9 7 1.63 1.27M12 18.54l-7.38-5.73L3 14.07l9 7 9-7-1.63-1.27L12 18.54z"/></svg>

    </a>
    ugo_py_doc
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ugoproto/ugo_py_doc.git/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ugo_py_doc
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Basics" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../py_cs/" class="md-nav__link">
        Python Cheat Sheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../databases/" class="md-nav__link">
        Databases
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../datetime/" class="md-nav__link">
        Datetime
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../decorators/" class="md-nav__link">
        Decorators
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exceptions/" class="md-nav__link">
        Exceptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../freeze_the_code/" class="md-nav__link">
        Freeze the Code
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../gedit_execute_highlighted_python_code/" class="md-nav__link">
        Gedit, Execute Highlighted Code
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../python_nice_to_have/" class="md-nav__link">
        Python Nice to Have
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Scipy Stack
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Scipy Stack" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Scipy Stack
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../jn_cs/" class="md-nav__link">
        Jupyter Notebook Cheat Sheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../scipy_cs/" class="md-nav__link">
        Scipy Stack Cheat Sheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../eda_machine_learning_feature_engineering_and_kaggle/" class="md-nav__link">
        EDA, Machine Learning, Feature Engineering, and Kaggle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exploratory_data_analysis/" class="md-nav__link">
        Exploratory Data Analysis (EDA)
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Feature Selection
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Feature Selection
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#a-feature-selection-case" class="md-nav__link">
    A feature selection case
  </a>
  
    <nav class="md-nav" aria-label="A feature selection case">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#filter-feature-selection-techniques" class="md-nav__link">
    Filter feature selection techniques
  </a>
  
    <nav class="md-nav" aria-label="Filter feature selection techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-r" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrapper-feature-selection-techniques" class="md-nav__link">
    Wrapper feature selection techniques
  </a>
  
    <nav class="md-nav" aria-label="Wrapper feature selection techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#another-dataset-another-example" class="md-nav__link">
    Another dataset, another example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-r_1" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedded-feature-selection-techniques" class="md-nav__link">
    Embedded feature selection techniques
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#takeway" class="md-nav__link">
    Takeway
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-penalize-the-magnitude-of-coefficients" class="md-nav__link">
    Why Penalize the Magnitude of Coefficients?
  </a>
  
    <nav class="md-nav" aria-label="Why Penalize the Magnitude of Coefficients?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regression" class="md-nav__link">
    Ridge regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regression" class="md-nav__link">
    Lasso regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-difference" class="md-nav__link">
    Key Difference
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-use-cases" class="md-nav__link">
    Typical Use Cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presence-of-highly-correlated-features" class="md-nav__link">
    Presence of Highly Correlated Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elasticnet-regression" class="md-nav__link">
    ElasticNet Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-reduction-techniques" class="md-nav__link">
    Data reduction techniques
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../geospatial_data_in_python/" class="md-nav__link">
        Geospatial Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../importing_data_into_python/" class="md-nav__link">
        Importing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../introduction_to_customer_segmentation_in_python/" class="md-nav__link">
        Introduction to Customer Segmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../intro_to_data_world_in_python/" class="md-nav__link">
        Introduction to data.world
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../overview_of_scikit_learn/" class="md-nav__link">
        Overview of scikit-learn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../python_and_excel/" class="md-nav__link">
        Python and Excel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../scaling_centering_noise_with_knn_linear_regression_logit/" class="md-nav__link">
        Scaling, Centering, Noise with kNN, Linear Regression, Logit
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../sentiment_analysis_with_twitter/" class="md-nav__link">
        Sentiment Analysis with Twitter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../time_series_analysis/" class="md-nav__link">
        Time Series Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../vectors_and_arrays_linear_algebra/" class="md-nav__link">
        Vectors and Arrays (Linear Algebra)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../viewing_3d_volumetric_data_with_matplotlib/" class="md-nav__link">
        Viewing 3D Volumetric Data with Matplotlib
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../write_idiomatic_pandas_code/" class="md-nav__link">
        Write Idiomatic Pandas Code
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Courses
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Courses" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Courses
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../apprenez_a_programmer_en_python/" class="md-nav__link">
        Apprenez à programmer en Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../automate_the_boring_stuff_with_python/" class="md-nav__link">
        Automate the Boring Stuff with Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../codecademy_python/" class="md-nav__link">
        Codecademy Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../learn_python_the_hard_way/" class="md-nav__link">
        Learn Python the Hard Way
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lpthw_python_code_snippets/" class="md-nav__link">
        LPTHW, Python Code Snippets
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Manuals
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Manuals" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Manuals
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../managing_your_biological_data_with_python/" class="md-nav__link">
        Managing Your Biological Data with Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../python_for_education/" class="md-nav__link">
        Python for Education
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#a-feature-selection-case" class="md-nav__link">
    A feature selection case
  </a>
  
    <nav class="md-nav" aria-label="A feature selection case">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#filter-feature-selection-techniques" class="md-nav__link">
    Filter feature selection techniques
  </a>
  
    <nav class="md-nav" aria-label="Filter feature selection techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-r" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrapper-feature-selection-techniques" class="md-nav__link">
    Wrapper feature selection techniques
  </a>
  
    <nav class="md-nav" aria-label="Wrapper feature selection techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#another-dataset-another-example" class="md-nav__link">
    Another dataset, another example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-r_1" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedded-feature-selection-techniques" class="md-nav__link">
    Embedded feature selection techniques
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#takeway" class="md-nav__link">
    Takeway
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-penalize-the-magnitude-of-coefficients" class="md-nav__link">
    Why Penalize the Magnitude of Coefficients?
  </a>
  
    <nav class="md-nav" aria-label="Why Penalize the Magnitude of Coefficients?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regression" class="md-nav__link">
    Ridge regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regression" class="md-nav__link">
    Lasso regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-difference" class="md-nav__link">
    Key Difference
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-use-cases" class="md-nav__link">
    Typical Use Cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presence-of-highly-correlated-features" class="md-nav__link">
    Presence of Highly Correlated Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elasticnet-regression" class="md-nav__link">
    ElasticNet Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-reduction-techniques" class="md-nav__link">
    Data reduction techniques
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ugoproto/ugo_py_doc.git/edit/master/docs/feature_selection_in_python.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  <h1>Feature Selection</h1>
                
                <h2 id="a-feature-selection-case">A feature selection case<a class="headerlink" href="#a-feature-selection-case" title="Permanent link">&para;</a></h2>
<p>We use the <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database">Pima Indians Diabetes dataset from Kaggle</a>.<br />
The dataset corresponds to classification tasks on which you need to predict if a person has diabetes based on 8 features.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Loading the primary modules</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Loading the data</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/diabetes.csv&quot;</span><span class="p">)</span>

<span class="c1"># Alternative way</span>
<span class="c1">#url = &quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv&quot;</span>
<span class="c1">#names = [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span>
<span class="c1">#dataframe = pd.read_csv(url, names=names)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># Renaming the features, fields or columns AND the response, dependent variable</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;preg&#39;</span><span class="p">,</span> <span class="s1">&#39;plas&#39;</span><span class="p">,</span> <span class="s1">&#39;pres&#39;</span><span class="p">,</span> <span class="s1">&#39;skin&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;pedi&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;class&#39;</span><span class="p">]</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">names</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preg</th>
      <th>plas</th>
      <th>pres</th>
      <th>skin</th>
      <th>test</th>
      <th>mass</th>
      <th>pedi</th>
      <th>age</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># Keeping the values only</span>
<span class="c1"># Converting the DataFrame object to a Numpy ndarray</span>
<span class="c1"># to achieve faster computation</span>
<span class="n">array</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Segregating the data into separate variables</span>
<span class="c1"># Features and the labels are separated</span>

<span class="c1"># Features, col 0 to 7</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span>
<span class="c1"># Response, col 8</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:,</span><span class="mi">8</span><span class="p">]</span>
</code></pre></div>
<h3 id="filter-feature-selection-techniques">Filter feature selection techniques<a class="headerlink" href="#filter-feature-selection-techniques" title="Permanent link">&para;</a></h3>
<p>Let&rsquo;s implement a chi-squared statistical test for non-negative features to select 4 of the best features from the dataset; from the scikit-learn module.  </p>
<p>Other &ldquo;Correlation&rdquo; techniques: Pearsons&rsquo; correlation, LDA, and ANOVA.</p>
<p><strong>A word on the chi-squared test</strong></p>
<p>The chi-squared test is used to determine whether there is a significant difference between the expected frequencies or proportions or distribution and the observed frequencies or proportions or distribution in one or more categories.<br />
It is used to compared the variance of categories or samples vs. population.<br />
It test if each category is mutually exclusive or statistically independent from the other categories.<br />
If the categories are independent, there are not &ldquo;correlated&rdquo;.<br />
The null hypothesis: mutually exclusive or statistically independent. When we reject the null hypothesis, we conclude to statistical dependence or homogeneity.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing the necessary modules</span>
<span class="c1"># SelectKBest class can be used with a suite of different statistical tests</span>
<span class="c1"># to select a specific number of features</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Feature extraction</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Summarizing scores</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>[  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393
   181.304]
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">scores_</span><span class="p">})</span>
</code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>preg</td>
      <td>111.519691</td>
    </tr>
    <tr>
      <th>1</th>
      <td>plas</td>
      <td>1411.887041</td>
    </tr>
    <tr>
      <th>2</th>
      <td>pres</td>
      <td>17.605373</td>
    </tr>
    <tr>
      <th>3</th>
      <td>skin</td>
      <td>53.108040</td>
    </tr>
    <tr>
      <th>4</th>
      <td>test</td>
      <td>2175.565273</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mass</td>
      <td>127.669343</td>
    </tr>
    <tr>
      <th>6</th>
      <td>pedi</td>
      <td>5.392682</td>
    </tr>
    <tr>
      <th>7</th>
      <td>age</td>
      <td>181.303689</td>
    </tr>
  </tbody>
</table>
</div>

<p>You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): <code>plas, test, mass, age</code>.<br />
This scores will help you further in determining the best features for training your model.</p>
<div class="highlight"><pre><span></span><code><span class="n">features</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Summarizing selected features (plas, test, mass, age)</span>
<span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:]</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>array([[ 148. ,    0. ,   33.6,   50. ],
       [  85. ,    0. ,   26.6,   31. ],
       [ 183. ,    0. ,   23.3,   32. ],
       [  89. ,   94. ,   28.1,   21. ],
       [ 137. ,  168. ,   43.1,   33. ]])
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;plas&#39;</span><span class="p">,</span><span class="s1">&#39;test&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;age&#39;</span><span class="p">])</span>
</code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>plas</th>
      <th>test</th>
      <th>mass</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>148.0</td>
      <td>0.0</td>
      <td>33.6</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>85.0</td>
      <td>0.0</td>
      <td>26.6</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>183.0</td>
      <td>0.0</td>
      <td>23.3</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>89.0</td>
      <td>94.0</td>
      <td>28.1</td>
      <td>21.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>137.0</td>
      <td>168.0</td>
      <td>43.1</td>
      <td>33.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># Original dataset</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preg</th>
      <th>plas</th>
      <th>pres</th>
      <th>skin</th>
      <th>test</th>
      <th>mass</th>
      <th>pedi</th>
      <th>age</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id="in-r">In R<a class="headerlink" href="#in-r" title="Permanent link">&para;</a></h4>
<p>The caret package provides tools to automatically report on the relevance and importance of attributes in your data and even select the most important features.<br />
Data can contain attributes that are highly correlated with each other.<br />
Many methods perform better if highly correlated attributes are removed.<br />
The caret package provides <code>findCorrelation</code> which will analyze a correlation matrix of your data’s attributes report on attributes that can be removed.<br />
Using the Pima Indians Diabetes dataset, let&rsquo;s remove attributes with an absolute correlation of 0.75 or higher.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Ensuring the results are repeatable</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlbench</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span>

<span class="c1"># Loading the data</span>
<span class="nf">data</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">)</span>

<span class="c1"># Calculating the correlation matrix</span>
<span class="n">correlationMatrix</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">])</span>

<span class="c1"># Summarizing the correlation matrix</span>
<span class="nf">print</span><span class="p">(</span><span class="n">correlationMatrix</span><span class="p">)</span>

<span class="c1"># Finding attributes that are highly corrected (ideally &gt;0.75)</span>
<span class="n">highlyCorrelated</span> <span class="o">&lt;-</span> <span class="nf">findCorrelation</span><span class="p">(</span><span class="n">correlationMatrix</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span>

<span class="c1"># Printing the indexes of highly correlated attributes</span>
<span class="nf">print</span><span class="p">(</span><span class="n">highlyCorrelated</span><span class="p">)</span><span class="n">r</span>
</code></pre></div>
<p>The importance of features can be estimated from data by building a model.<br />
Some methods like decision trees have a built-in mechanism to report on variable importance.<br />
For other algorithms, the importance can be estimated using a ROC curve analysis conducted for each attribute.<br />
Let&rsquo;s constructs a Learning Vector Quantization (LVQ) model.  </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Ensuring the results are repeatable</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlbench</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span>

<span class="c1"># Loading the data</span>
<span class="nf">data</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">)</span>

<span class="c1"># Calculating the correlation matrix</span>
<span class="n">correlationMatrix</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">])</span>

<span class="c1"># Summarizing the correlation matrix</span>
<span class="nf">print</span><span class="p">(</span><span class="n">correlationMatrix</span><span class="p">)</span>

<span class="c1"># Finding attributes that are highly corrected (ideally &gt; 0.75)</span>
<span class="n">highlyCorrelated</span> <span class="o">&lt;-</span> <span class="nf">findCorrelation</span><span class="p">(</span><span class="n">correlationMatrix</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span>

<span class="c1"># Printing indexes of highly correlated attributes</span>
<span class="nf">print</span><span class="p">(</span><span class="n">highlyCorrelated</span><span class="p">)</span>
</code></pre></div>
<h3 id="wrapper-feature-selection-techniques">Wrapper feature selection techniques<a class="headerlink" href="#wrapper-feature-selection-techniques" title="Permanent link">&para;</a></h3>
<p>Let&rsquo;s implement a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE">Recursive Feature Elimination</a> from the scikit-learn module.</p>
<p>Other techniques: Forward Selection, Backward Elimination, and Combination of forward selection and backward elimination.</p>
<p>The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.<br />
It uses the model accuracy to identify which attributes (and a combination of attributes) contribute the most to predicting the target attribute.<br />
You use RFE with the Logistic Regression classifier to select the top 3 features.<br />
The choice of algorithms does not matter too much as long as it is consistent.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Feature extraction</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Summarizing the selection of the attributes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature Ranking: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">))</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Num Features: 3
Selected Features: [ True False False False False  True  True False]
Feature Ranking: [1 2 3 5 6 1 1 4]


/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;selected&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">})</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>ranking</th>
      <th>selected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>preg</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>plas</td>
      <td>2</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>pres</td>
      <td>3</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>skin</td>
      <td>5</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>test</td>
      <td>6</td>
      <td>False</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mass</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>pedi</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>7</th>
      <td>age</td>
      <td>4</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<p>You can see that RFE chose the top 3 features as <code>preg, mass, pedi</code>.<br />
These are marked True in the support array and marked with a choice “1” in the ranking array.</p>
<p>You can also use RFE with the Bagged decision trees like Random Forest and Extra Trees to estimate the importance of features.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Feature extraction</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Summarizing the selection of the attributes</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>[0.10490793 0.20041541 0.0958495  0.08070628 0.0849718  0.15844774
 0.12226854 0.15243279]


/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features Importance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Num Features: 8
Features Importance: [0.10490793 0.20041541 0.0958495  0.08070628 0.0849718  0.15844774
 0.12226854 0.15243279]
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">})</span><span class="o">.</span>\
    <span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
      <th>names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.200415</td>
      <td>plas</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.158448</td>
      <td>mass</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.152433</td>
      <td>age</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.122269</td>
      <td>pedi</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.104908</td>
      <td>preg</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.095849</td>
      <td>pres</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.084972</td>
      <td>test</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.080706</td>
      <td>skin</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id="another-dataset-another-example">Another dataset, another example<a class="headerlink" href="#another-dataset-another-example" title="Permanent link">&para;</a></h4>
<p>Let&rsquo;s tackle another example using the built-in iris dataset, reusing the Logistic Regression and the Extra Tree Ensemble.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Loading the iris datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span><span class="s1">&#39;sepal width&#39;</span><span class="p">,</span><span class="s1">&#39;petal length&#39;</span><span class="p">,</span><span class="s1">&#39;petal width&#39;</span><span class="p">]</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<span class="n">dataset_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># Creating a base classifier used to evaluate a subset of attributes</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Creating the RFE model and select 3 attributes</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature Ranking: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">))</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Num Features: 3
Selected Features: [False  True  True  True]
Feature Ranking: [2 1 1 1]
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">,</span> <span class="s1">&#39;selected&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">})</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>ranking</th>
      <th>selected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>sepal length</td>
      <td>2</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>sepal width</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>petal length</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>petal width</td>
      <td>1</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>

<p>Keep top-ranking features (rank 1) and leave out the other features (rank 2).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Creating a base classifier</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>

<span class="c1"># Creating the RFE model and select 3 attributes</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="k">[0.08470961 0.02095061 0.37336503 0.52097475]</span>


<span class="na">/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.</span>
  <span class="na">&quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)</span>
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features Importance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rfe</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Num Features: 4
Features Importance: [0.08470961 0.02095061 0.37336503 0.52097475]
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">rfe</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">})</span><span class="o">.</span>\
    <span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
      <th>names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>0.520975</td>
      <td>petal width</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.373365</td>
      <td>petal length</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.084710</td>
      <td>sepal length</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.020951</td>
      <td>sepal width</td>
    </tr>
  </tbody>
</table>
</div>

<p>The last results confirm the previous results.</p>
<h4 id="in-r_1">In R<a class="headerlink" href="#in-r_1" title="Permanent link">&para;</a></h4>
<p>Recursive Feature Elimination or RFE.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Ensuring the results are repeatable</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlbench</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span>

<span class="c1"># Loading the data</span>
<span class="nf">data</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">)</span>

<span class="c1"># Defining the control using a random forest selection function</span>
<span class="n">control</span> <span class="o">&lt;-</span> <span class="nf">rfeControl</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">rfFuncs</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&quot;cv&quot;</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="m">10</span><span class="p">)</span>

<span class="c1"># Running the RFE algorithm</span>
<span class="n">results</span> <span class="o">&lt;-</span> <span class="nf">rfe</span><span class="p">(</span><span class="n">PimaIndiansDiabetes</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">],</span> <span class="n">PimaIndiansDiabetes</span><span class="p">[,</span><span class="m">9</span><span class="p">],</span> <span class="n">sizes</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">),</span> <span class="n">rfeControl</span><span class="o">=</span><span class="n">control</span><span class="p">)</span>

<span class="c1"># Summarize the results</span>
<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Listing the chosen features</span>
<span class="nf">predictors</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Plotting the results</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;g&quot;</span><span class="p">,</span> <span class="s">&quot;o&quot;</span><span class="p">))</span>
</code></pre></div>
<h3 id="embedded-feature-selection-techniques">Embedded feature selection techniques<a class="headerlink" href="#embedded-feature-selection-techniques" title="Permanent link">&para;</a></h3>
<p>Let&rsquo;s use the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge regression</a> from the scikit-learn module; a regularization technique as well.</p>
<p>Other techniques: LASSO and Elastic Net.</p>
<p>Find out more about <a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#thre">regularization techniques</a>.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing your necessary module</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Using Ridge regression to determine the R-squared</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)
</code></pre></div>
</td></tr></table>
<p>In order to better understand the results of Ridge regression, you will implement a little helper function that will help you to print the results in a better so that you can interpret them easily.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Implementing a function for pretty-printing the coefficients</span>
<span class="k">def</span> <span class="nf">pretty_print_coefs</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">names</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sort</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">names</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">))]</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sort</span><span class="p">:</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span>  <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">return</span> <span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> * </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">name</span><span class="p">)</span>
                                   <span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Applying the function</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="c1"># Applying the function with the names</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">names</span><span class="p">))</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">Ridge</span> <span class="nl">model:</span> <span class="mf">0.021</span> <span class="o">*</span> <span class="n">X0</span> <span class="o">+</span> <span class="mf">0.006</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.002</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">+</span> <span class="mf">0.0</span> <span class="o">*</span> <span class="n">X3</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.0</span> <span class="o">*</span> <span class="n">X4</span> <span class="o">+</span> <span class="mf">0.013</span> <span class="o">*</span> <span class="n">X5</span> <span class="o">+</span> <span class="mf">0.145</span> <span class="o">*</span> <span class="n">X6</span> <span class="o">+</span> <span class="mf">0.003</span> <span class="o">*</span> <span class="n">X7</span>

<span class="n">Ridge</span> <span class="nl">model:</span> <span class="mf">0.021</span> <span class="o">*</span> <span class="n">preg</span> <span class="o">+</span> <span class="mf">0.006</span> <span class="o">*</span> <span class="n">plas</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.002</span> <span class="o">*</span> <span class="n">pres</span> <span class="o">+</span> <span class="mf">0.0</span> <span class="o">*</span> <span class="n">skin</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.0</span> <span class="o">*</span> <span class="n">test</span> <span class="o">+</span> <span class="mf">0.013</span> <span class="o">*</span> <span class="n">mass</span> <span class="o">+</span> <span class="mf">0.145</span> <span class="o">*</span> <span class="n">pedi</span> <span class="o">+</span> <span class="mf">0.003</span> <span class="o">*</span> <span class="n">age</span>
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="c1"># Applying the function with the names and sorting the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">Ridge</span> <span class="nl">model:</span> <span class="mf">0.145</span> <span class="o">*</span> <span class="n">pedi</span> <span class="o">+</span> <span class="mf">0.021</span> <span class="o">*</span> <span class="n">preg</span> <span class="o">+</span> <span class="mf">0.013</span> <span class="o">*</span> <span class="n">mass</span> <span class="o">+</span> <span class="mf">0.006</span> <span class="o">*</span> <span class="n">plas</span> <span class="o">+</span> <span class="mf">0.003</span> <span class="o">*</span> <span class="n">age</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.002</span> <span class="o">*</span> <span class="n">pres</span> <span class="o">+</span> <span class="o">-</span><span class="mf">0.0</span> <span class="o">*</span> <span class="n">test</span> <span class="o">+</span> <span class="mf">0.0</span> <span class="o">*</span> <span class="n">skin</span>
</code></pre></div>
</td></tr></table>
<p>You can spot all the coefficient terms appended with the feature variables.<br />
You can pick the most essential features.<br />
The sorted top 3 features are <code>pedi, preg, mass</code>.  </p>
<p><strong>A word on the Ridge regression</strong></p>
<ul>
<li>It is also known as L2-Regularization.</li>
<li>For correlated features, it means that they tend to get similar coefficients.</li>
<li>Feature having negative coefficients don&rsquo;t contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision-making process.</li>
</ul>
<h3 id="takeway">Takeway<a class="headerlink" href="#takeway" title="Permanent link">&para;</a></h3>
<p>The three techniques help to understand the features of a particular dataset in a comprehensive manner.<br />
Feature selection is essentially a part of <strong>data preprocessing</strong> which is considered to be the most time-consuming part of any machine learning pipeline.<br />
These techniques will help you to approach it in a more systematic way and machine learning friendly way.<br />
You will be able to interpret the features more accurately.</p>
<h2 id="why-penalize-the-magnitude-of-coefficients">Why Penalize the Magnitude of Coefficients?<a class="headerlink" href="#why-penalize-the-magnitude-of-coefficients" title="Permanent link">&para;</a></h2>
<p>Given a sine curve (between 60° and 300°) and some random noise using the following code:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing modules</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">matplotlib.pylab</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">10</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Defining input array with angles</span>
<span class="c1"># from 60deg to 300deg converted to radians</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">4</span><span class="p">)])</span>

<span class="c1"># Setting seed for reproducability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]),</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">);</span>
</code></pre></div>
<p><img alt="" src="../img/Feature_Selection_in_Python/output_45_0.png" /></p>
<p>Let&rsquo;s try to estimate the sine function using polynomial regression with powers of x form 1 to 15.<br />
Let&rsquo;s add a column for each power up to 15.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Power of 1 is already there</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">):</span>  
    <span class="n">colname</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="c1"># new var will be x_power</span>
    <span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">**</span><span class="n">i</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>x_2</th>
      <th>x_3</th>
      <th>x_4</th>
      <th>x_5</th>
      <th>x_6</th>
      <th>x_7</th>
      <th>x_8</th>
      <th>x_9</th>
      <th>x_10</th>
      <th>x_11</th>
      <th>x_12</th>
      <th>x_13</th>
      <th>x_14</th>
      <th>x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.047198</td>
      <td>1.065763</td>
      <td>1.096623</td>
      <td>1.148381</td>
      <td>1.202581</td>
      <td>1.259340</td>
      <td>1.318778</td>
      <td>1.381021</td>
      <td>1.446202</td>
      <td>1.514459</td>
      <td>1.585938</td>
      <td>1.660790</td>
      <td>1.739176</td>
      <td>1.821260</td>
      <td>1.907219</td>
      <td>1.997235</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.117011</td>
      <td>1.006086</td>
      <td>1.247713</td>
      <td>1.393709</td>
      <td>1.556788</td>
      <td>1.738948</td>
      <td>1.942424</td>
      <td>2.169709</td>
      <td>2.423588</td>
      <td>2.707173</td>
      <td>3.023942</td>
      <td>3.377775</td>
      <td>3.773011</td>
      <td>4.214494</td>
      <td>4.707635</td>
      <td>5.258479</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.186824</td>
      <td>0.695374</td>
      <td>1.408551</td>
      <td>1.671702</td>
      <td>1.984016</td>
      <td>2.354677</td>
      <td>2.794587</td>
      <td>3.316683</td>
      <td>3.936319</td>
      <td>4.671717</td>
      <td>5.544505</td>
      <td>6.580351</td>
      <td>7.809718</td>
      <td>9.268760</td>
      <td>11.000386</td>
      <td>13.055521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.256637</td>
      <td>0.949799</td>
      <td>1.579137</td>
      <td>1.984402</td>
      <td>2.493673</td>
      <td>3.133642</td>
      <td>3.937850</td>
      <td>4.948448</td>
      <td>6.218404</td>
      <td>7.814277</td>
      <td>9.819710</td>
      <td>12.339811</td>
      <td>15.506664</td>
      <td>19.486248</td>
      <td>24.487142</td>
      <td>30.771450</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.326450</td>
      <td>1.063496</td>
      <td>1.759470</td>
      <td>2.333850</td>
      <td>3.095735</td>
      <td>4.106339</td>
      <td>5.446854</td>
      <td>7.224981</td>
      <td>9.583578</td>
      <td>12.712139</td>
      <td>16.862020</td>
      <td>22.366630</td>
      <td>29.668222</td>
      <td>39.353420</td>
      <td>52.200353</td>
      <td>69.241170</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now that we have all the 15 powers, let&rsquo;s make 15 different linear regression models with each model containing variables with powers of x from 1 to the particular model number.<br />
For example, the feature set of model 8 will be {x, x_2, x_3, &hellip; , x_8}.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing the Linear Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="p">):</span>
    <span class="c1">#initialize predictors:</span>
    <span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">power</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">power</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>

    <span class="c1"># Fitting the model</span>
    <span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1"># Checking if a plot is to be made for the entered power</span>
    <span class="k">if</span> <span class="n">power</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">power</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for power: </span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">power</span><span class="p">)</span>

    <span class="c1"># Returning the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</code></pre></div>
<p>Now, we can make all 15 models and compare the results.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initializing a DataFrame to store the results</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;model_pow_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">coef_matrix_simple</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Defining the powers for which a plot is required</span>
<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span><span class="mi">3</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span><span class="mi">6</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span><span class="mi">9</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span><span class="mi">12</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span><span class="mi">15</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Iterating through all powers and assimilate results</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">coef_matrix_simple</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="n">models_to_plot</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="../img/Feature_Selection_in_Python/output_54_0.png" /></p>
<p>As the model complexity increases, the models tend to fit even smaller deviations in the training data set.  </p>
<p>This leads to overfitting.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{:,.2g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_simple</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>model_pow_1</th>
      <td>3.3</td>
      <td>2</td>
      <td>-0.62</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_2</th>
      <td>3.3</td>
      <td>1.9</td>
      <td>-0.58</td>
      <td>-0.006</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_3</th>
      <td>1.1</td>
      <td>-1.1</td>
      <td>3</td>
      <td>-1.3</td>
      <td>0.14</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_4</th>
      <td>1.1</td>
      <td>-0.27</td>
      <td>1.7</td>
      <td>-0.53</td>
      <td>-0.036</td>
      <td>0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_5</th>
      <td>1</td>
      <td>3</td>
      <td>-5.1</td>
      <td>4.7</td>
      <td>-1.9</td>
      <td>0.33</td>
      <td>-0.021</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_6</th>
      <td>0.99</td>
      <td>-2.8</td>
      <td>9.5</td>
      <td>-9.7</td>
      <td>5.2</td>
      <td>-1.6</td>
      <td>0.23</td>
      <td>-0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_7</th>
      <td>0.93</td>
      <td>19</td>
      <td>-56</td>
      <td>69</td>
      <td>-45</td>
      <td>17</td>
      <td>-3.5</td>
      <td>0.4</td>
      <td>-0.019</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_8</th>
      <td>0.92</td>
      <td>43</td>
      <td>-1.4e+02</td>
      <td>1.8e+02</td>
      <td>-1.3e+02</td>
      <td>58</td>
      <td>-15</td>
      <td>2.4</td>
      <td>-0.21</td>
      <td>0.0077</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_9</th>
      <td>0.87</td>
      <td>1.7e+02</td>
      <td>-6.1e+02</td>
      <td>9.6e+02</td>
      <td>-8.5e+02</td>
      <td>4.6e+02</td>
      <td>-1.6e+02</td>
      <td>37</td>
      <td>-5.2</td>
      <td>0.42</td>
      <td>-0.015</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_10</th>
      <td>0.87</td>
      <td>1.4e+02</td>
      <td>-4.9e+02</td>
      <td>7.3e+02</td>
      <td>-6e+02</td>
      <td>2.9e+02</td>
      <td>-87</td>
      <td>15</td>
      <td>-0.81</td>
      <td>-0.14</td>
      <td>0.026</td>
      <td>-0.0013</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_11</th>
      <td>0.87</td>
      <td>-75</td>
      <td>5.1e+02</td>
      <td>-1.3e+03</td>
      <td>1.9e+03</td>
      <td>-1.6e+03</td>
      <td>9.1e+02</td>
      <td>-3.5e+02</td>
      <td>91</td>
      <td>-16</td>
      <td>1.8</td>
      <td>-0.12</td>
      <td>0.0034</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_12</th>
      <td>0.87</td>
      <td>-3.4e+02</td>
      <td>1.9e+03</td>
      <td>-4.4e+03</td>
      <td>6e+03</td>
      <td>-5.2e+03</td>
      <td>3.1e+03</td>
      <td>-1.3e+03</td>
      <td>3.8e+02</td>
      <td>-80</td>
      <td>12</td>
      <td>-1.1</td>
      <td>0.062</td>
      <td>-0.0016</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_13</th>
      <td>0.86</td>
      <td>3.2e+03</td>
      <td>-1.8e+04</td>
      <td>4.5e+04</td>
      <td>-6.7e+04</td>
      <td>6.6e+04</td>
      <td>-4.6e+04</td>
      <td>2.3e+04</td>
      <td>-8.5e+03</td>
      <td>2.3e+03</td>
      <td>-4.5e+02</td>
      <td>62</td>
      <td>-5.7</td>
      <td>0.31</td>
      <td>-0.0078</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_14</th>
      <td>0.79</td>
      <td>2.4e+04</td>
      <td>-1.4e+05</td>
      <td>3.8e+05</td>
      <td>-6.1e+05</td>
      <td>6.6e+05</td>
      <td>-5e+05</td>
      <td>2.8e+05</td>
      <td>-1.2e+05</td>
      <td>3.7e+04</td>
      <td>-8.5e+03</td>
      <td>1.5e+03</td>
      <td>-1.8e+02</td>
      <td>15</td>
      <td>-0.73</td>
      <td>0.017</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_15</th>
      <td>0.7</td>
      <td>-3.6e+04</td>
      <td>2.4e+05</td>
      <td>-7.5e+05</td>
      <td>1.4e+06</td>
      <td>-1.7e+06</td>
      <td>1.5e+06</td>
      <td>-1e+06</td>
      <td>5e+05</td>
      <td>-1.9e+05</td>
      <td>5.4e+04</td>
      <td>-1.2e+04</td>
      <td>1.9e+03</td>
      <td>-2.2e+02</td>
      <td>17</td>
      <td>-0.81</td>
      <td>0.018</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is clearly evident that the size of coefficients increase exponentially with increase in model complexity.<br />
It means that we&rsquo;re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome.<br />
When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data.</p>
<h3 id="ridge-regression">Ridge regression<a class="headerlink" href="#ridge-regression" title="Permanent link">&para;</a></h3>
<p>A ridge regression performs L2 regularization&rsquo;, i.e. it adds a factor of sum of squares of coefficients in the optimization objective.</p>
<p><strong>Objective = RSS + alpha * (sum of square of coefficients)</strong></p>
<p>The alpha parameter balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients.</p>
<ul>
<li>alpha = 0, a simple linear regression;</li>
<li>alpha = infinite, infinite weight on square of coefficients, anything less than zero will make the objective infinite; all coefficients zero;</li>
<li>0 &lt; alpha &lt; infinite, somewhere between 0 and 1 for simple linear regression.</li>
</ul>
<p>One thing is for sure that any non-zero value would give values less than that of simple linear regression.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing the Ridge Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">ridge_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="p">{}):</span>
    <span class="c1"># Fitting the model</span>
    <span class="n">ridgereg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ridgereg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridgereg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1"># Checking if a plot is to be made for the entered alpha</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">alpha</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for alpha: </span><span class="si">%.3g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha</span><span class="p">)</span>

    <span class="c1"># Returning the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ridgereg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ridgereg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</code></pre></div>
<p>Let&rsquo;s analyze the result of Ridge regression for 10 different values of alpha ranging from 1e-15 to 20.<br />
Each of these 10 models will contain all the 15 variables and only the value of alpha would differ.<br />
This is different from the simple linear regression case where each model had a subset of features.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initializing predictors to be set of 15 powers of x</span>
<span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)])</span>

<span class="c1"># Setting the different values of alpha to be tested</span>
<span class="n">alpha_ridge</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-15</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Initializing the dataframe for storing coefficients.</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha_</span><span class="si">%.2g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha_ridge</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">coef_matrix_ridge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1e-15</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">coef_matrix_ridge</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,]</span> <span class="o">=</span> <span class="n">ridge_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha_ridge</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">models_to_plot</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:125: LinAlgWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number4.572933e-17
  overwrite_a=True).T
</code></pre></div>
</td></tr></table>
<p><img alt="" src="../img/Feature_Selection_in_Python/output_61_1.png" /></p>
<p>As the value of alpha increases, the model complexity reduces.<br />
Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (alpha = 5, for example).<br />
Thus alpha should be chosen wisely.<br />
A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{:,.2g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_ridge</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha_1e-15</th>
      <td>0.87</td>
      <td>94</td>
      <td>-3e+02</td>
      <td>3.7e+02</td>
      <td>-2.3e+02</td>
      <td>61</td>
      <td>2.6</td>
      <td>-5.1</td>
      <td>0.62</td>
      <td>0.17</td>
      <td>-0.03</td>
      <td>-0.0052</td>
      <td>0.001</td>
      <td>0.00017</td>
      <td>-4.6e-05</td>
      <td>3.1e-06</td>
      <td>-3.8e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-10</th>
      <td>0.92</td>
      <td>11</td>
      <td>-29</td>
      <td>31</td>
      <td>-15</td>
      <td>2.9</td>
      <td>0.17</td>
      <td>-0.091</td>
      <td>-0.011</td>
      <td>0.002</td>
      <td>0.00064</td>
      <td>2.4e-05</td>
      <td>-2e-05</td>
      <td>-4.2e-06</td>
      <td>2.2e-07</td>
      <td>2.3e-07</td>
      <td>-2.3e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-08</th>
      <td>0.95</td>
      <td>1.3</td>
      <td>-1.5</td>
      <td>1.7</td>
      <td>-0.68</td>
      <td>0.039</td>
      <td>0.016</td>
      <td>0.00016</td>
      <td>-0.00036</td>
      <td>-5.4e-05</td>
      <td>-2.9e-07</td>
      <td>1.1e-06</td>
      <td>1.9e-07</td>
      <td>2e-08</td>
      <td>3.9e-09</td>
      <td>8.2e-10</td>
      <td>-4.6e-10</td>
    </tr>
    <tr>
      <th>alpha_0.0001</th>
      <td>0.96</td>
      <td>0.56</td>
      <td>0.55</td>
      <td>-0.13</td>
      <td>-0.026</td>
      <td>-0.0028</td>
      <td>-0.00011</td>
      <td>4.1e-05</td>
      <td>1.5e-05</td>
      <td>3.7e-06</td>
      <td>7.4e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.9e-09</td>
      <td>-1.3e-10</td>
      <td>-1.5e-10</td>
      <td>-6.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.001</th>
      <td>1</td>
      <td>0.82</td>
      <td>0.31</td>
      <td>-0.087</td>
      <td>-0.02</td>
      <td>-0.0028</td>
      <td>-0.00022</td>
      <td>1.8e-05</td>
      <td>1.2e-05</td>
      <td>3.4e-06</td>
      <td>7.3e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.7e-09</td>
      <td>-1.5e-10</td>
      <td>-1.4e-10</td>
      <td>-5.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.01</th>
      <td>1.4</td>
      <td>1.3</td>
      <td>-0.088</td>
      <td>-0.052</td>
      <td>-0.01</td>
      <td>-0.0014</td>
      <td>-0.00013</td>
      <td>7.2e-07</td>
      <td>4.1e-06</td>
      <td>1.3e-06</td>
      <td>3e-07</td>
      <td>5.6e-08</td>
      <td>9e-09</td>
      <td>1.1e-09</td>
      <td>4.3e-11</td>
      <td>-3.1e-11</td>
      <td>-1.5e-11</td>
    </tr>
    <tr>
      <th>alpha_1</th>
      <td>5.6</td>
      <td>0.97</td>
      <td>-0.14</td>
      <td>-0.019</td>
      <td>-0.003</td>
      <td>-0.00047</td>
      <td>-7e-05</td>
      <td>-9.9e-06</td>
      <td>-1.3e-06</td>
      <td>-1.4e-07</td>
      <td>-9.3e-09</td>
      <td>1.3e-09</td>
      <td>7.8e-10</td>
      <td>2.4e-10</td>
      <td>6.2e-11</td>
      <td>1.4e-11</td>
      <td>3.2e-12</td>
    </tr>
    <tr>
      <th>alpha_5</th>
      <td>14</td>
      <td>0.55</td>
      <td>-0.059</td>
      <td>-0.0085</td>
      <td>-0.0014</td>
      <td>-0.00024</td>
      <td>-4.1e-05</td>
      <td>-6.9e-06</td>
      <td>-1.1e-06</td>
      <td>-1.9e-07</td>
      <td>-3.1e-08</td>
      <td>-5.1e-09</td>
      <td>-8.2e-10</td>
      <td>-1.3e-10</td>
      <td>-2e-11</td>
      <td>-3e-12</td>
      <td>-4.2e-13</td>
    </tr>
    <tr>
      <th>alpha_10</th>
      <td>18</td>
      <td>0.4</td>
      <td>-0.037</td>
      <td>-0.0055</td>
      <td>-0.00095</td>
      <td>-0.00017</td>
      <td>-3e-05</td>
      <td>-5.2e-06</td>
      <td>-9.2e-07</td>
      <td>-1.6e-07</td>
      <td>-2.9e-08</td>
      <td>-5.1e-09</td>
      <td>-9.1e-10</td>
      <td>-1.6e-10</td>
      <td>-2.9e-11</td>
      <td>-5.1e-12</td>
      <td>-9.1e-13</td>
    </tr>
    <tr>
      <th>alpha_20</th>
      <td>23</td>
      <td>0.28</td>
      <td>-0.022</td>
      <td>-0.0034</td>
      <td>-0.0006</td>
      <td>-0.00011</td>
      <td>-2e-05</td>
      <td>-3.6e-06</td>
      <td>-6.6e-07</td>
      <td>-1.2e-07</td>
      <td>-2.2e-08</td>
      <td>-4e-09</td>
      <td>-7.5e-10</td>
      <td>-1.4e-10</td>
      <td>-2.5e-11</td>
      <td>-4.7e-12</td>
      <td>-8.7e-13</td>
    </tr>
  </tbody>
</table>
</div>

<p>The RSS increases with increases in alpha, this model complexity reduces.<br />
An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients.<br />
High alpha values can lead to significant underfitting.<br />
Note the rapid increase in RSS for values of alpha greater than 1.<br />
Though the coefficients are very very small, they are NOT zero.<br />
Let’s reconfirm the same by determining the number of zeros in each row of the coefficients dataset.</p>
<div class="highlight"><pre><span></span><code><span class="n">coef_matrix_ridge</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>alpha_1e-15     0
alpha_1e-10     0
alpha_1e-08     0
alpha_0.0001    0
alpha_0.001     0
alpha_0.01      0
alpha_1         0
alpha_5         0
alpha_10        0
alpha_20        0
dtype: int64
</code></pre></div>
</td></tr></table>
<h3 id="lasso-regression">Lasso regression<a class="headerlink" href="#lasso-regression" title="Permanent link">&para;</a></h3>
<p>LASSO stands for <em>Least Absolute Shrinkage and Selection Operator</em>.<br />
Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective.  </p>
<p><strong>Objective = RSS + alpha * (sum of absolute value of coefficients)</strong></p>
<p>The alpha works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients.</p>
<ul>
<li>alpha = 0, a simple linear regression;</li>
<li>alpha = infinite, infinite weight on square of coefficients, anything less than zero will make the objective infinite; all coefficients zero.</li>
<li>0 &lt; alpha &lt; infinite, somewhere between 0 and 1 for simple linear regression.</li>
</ul>
<p>It appears to be very similar to the Ridge regression.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing the Ridge Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">lasso_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="p">{}):</span>
    <span class="c1">#Fit the model</span>
    <span class="n">lassoreg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>
    <span class="n">lassoreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lassoreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1">#Check if a plot is to be made for the entered alpha</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">alpha</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for alpha: </span><span class="si">%.3g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha</span><span class="p">)</span>

    <span class="c1">#Return the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">lassoreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lassoreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</code></pre></div>
<p>The additional parameters defined in Lasso function <code>max_iter</code> is the maximum number of iterations for which we want the model to run if it doesn&rsquo;t converge before.<br />
This exists for Ridge regression as well but setting this to a higher than default value was required in this case.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initializing predictors to all 15 powers of x</span>
<span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)])</span>

<span class="c1"># Defining the alpha values to test</span>
<span class="n">alpha_lasso</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-15</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="c1"># Initializing the dataframe to store coefficients</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha_</span><span class="si">%.2g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha_lasso</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">coef_matrix_lasso</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="c1"># Defining the models to plot</span>
<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1e-10</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span><span class="mf">1e-4</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>

<span class="c1"># Iterating over the 10 alpha values:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">coef_matrix_lasso</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,]</span> <span class="o">=</span> <span class="n">lasso_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha_lasso</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">models_to_plot</span><span class="p">);</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
</code></pre></div>
</td></tr></table>
<p><img alt="" src="../img/Feature_Selection_in_Python/output_70_1.png" /></p>
<p>This again tells us that the model complexity decreases with increases in the values of alpha.<br />
But notice the straight line at alpha=1.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{:,.2g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_lasso</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha_1e-15</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00089</td>
      <td>0.0016</td>
      <td>-0.00012</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.4e-09</td>
      <td>1.8e-10</td>
      <td>-2e-10</td>
      <td>-9.2e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-10</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00088</td>
      <td>0.0016</td>
      <td>-0.00012</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.4e-09</td>
      <td>1.8e-10</td>
      <td>-2e-10</td>
      <td>-9.2e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-08</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00077</td>
      <td>0.0016</td>
      <td>-0.00011</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.3e-09</td>
      <td>2e-10</td>
      <td>-1.9e-10</td>
      <td>-9.3e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-05</th>
      <td>0.96</td>
      <td>0.5</td>
      <td>0.6</td>
      <td>-0.13</td>
      <td>-0.038</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7e-06</td>
      <td>1e-06</td>
      <td>7.7e-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0</td>
      <td>-7e-11</td>
    </tr>
    <tr>
      <th>alpha_0.0001</th>
      <td>1</td>
      <td>0.9</td>
      <td>0.17</td>
      <td>-0</td>
      <td>-0.048</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>9.5e-06</td>
      <td>5.1e-07</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-4.4e-11</td>
    </tr>
    <tr>
      <th>alpha_0.001</th>
      <td>1.7</td>
      <td>1.3</td>
      <td>-0</td>
      <td>-0.13</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1.5e-08</td>
      <td>7.5e-10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>alpha_0.01</th>
      <td>3.6</td>
      <td>1.8</td>
      <td>-0.55</td>
      <td>-0.00056</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>alpha_1</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
    <tr>
      <th>alpha_5</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
    <tr>
      <th>alpha_10</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Apart from the expected inference of higher RSS for higher alphas, we can see the following:</p>
<ul>
<li>For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables);</li>
<li>For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression;</li>
<li>Many of the coefficients are zero even for very small values of alpha.</li>
</ul>
<p>The real difference from ridge is coming out in the last inference.<br />
Let&rsquo;s check the number of coefficients which are zero in each model using the following code.</p>
<div class="highlight"><pre><span></span><code><span class="n">coef_matrix_lasso</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>alpha_1e-15      0
alpha_1e-10      0
alpha_1e-08      0
alpha_1e-05      8
alpha_0.0001    10
alpha_0.001     12
alpha_0.01      13
alpha_1         15
alpha_5         15
alpha_10        15
dtype: int64
</code></pre></div>
</td></tr></table>
<p>For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables).<br />
For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression.<br />
Many of the coefficients are zero even for very small values of alpha.<br />
The real difference from ridge is coming out in the last inference.<br />
We can observe that even for a small value of alpha, a significant number of coefficients are zero.<br />
This also explains the horizontal line fit for alpha=1 in the lasso plots, it&rsquo;s just a baseline model!<br />
This phenomenon of most of the coefficients being zero is called &lsquo;sparsity&rsquo;. Although lasso performs feature selection, this level of sparsity is achieved in special cases only which we&rsquo;ll discuss towards the end.</p>
<h3 id="key-difference">Key Difference<a class="headerlink" href="#key-difference" title="Permanent link">&para;</a></h3>
<ul>
<li>Ridge: it includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.</li>
<li>Lasso: along with shrinking coefficients, lasso performs feature selection as well. (Remember the &lsquo;selection&rsquo; in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.</li>
</ul>
<p>Traditionally, techniques like stepwise regression were used to perform feature selection and make parsimonious models.<br />
But with advancements in Machine Learning, ridge and lasso regression provide very good alternatives as they give much better output, require fewer tuning parameters and can be automated to a large extent.</p>
<h3 id="typical-use-cases">Typical Use Cases<a class="headerlink" href="#typical-use-cases" title="Permanent link">&para;</a></h3>
<ul>
<li>Ridge: it is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of an exorbitantly high number of features, say in millions, as it will pose computational challenges.</li>
<li>Lasso: since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the number of features is in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.</li>
</ul>
<p>It&rsquo;s not hard to see why the stepwise selection techniques become practically very cumbersome to implement in high dimensionality cases.<br />
Thus, lasso provides a significant advantage.</p>
<h3 id="presence-of-highly-correlated-features">Presence of Highly Correlated Features<a class="headerlink" href="#presence-of-highly-correlated-features" title="Permanent link">&para;</a></h3>
<ul>
<li>Ridge: it generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.</li>
<li>Lasso: it arbitrarily selects any one feature among the highly correlated ones and reduced the coefficients of the rest to zero. Also, the chosen variable changes randomly with changes in model parameters. This generally doesn&rsquo;t work that well as compared to ridge regression.</li>
</ul>
<p>This disadvantage of lasso can be observed in the example we discussed above.<br />
Since we used a polynomial regression, the variables were highly correlated. (Not sure why? Check the output of <code>data.corr()</code>).<br />
Thus, we saw that even small values of alpha were giving significant sparsity (i.e. high numbers of coefficients as zero).</p>
<p>Along with Ridge and Lasso, Elastic Net is another useful technique which combines both L1 and L2 regularization. It can be used to balance out the pros and cons of ridge and lasso regression. I encourage you to explore it further.</p>
<h3 id="elasticnet-regression">ElasticNet Regression<a class="headerlink" href="#elasticnet-regression" title="Permanent link">&para;</a></h3>
<p>ElasticNet is hybrid of Lasso and Ridge Regression techniques.<br />
It is trained with L1 and L2 prior as regularizers.<br />
Elastic-net is useful when there are multiple features which are correlated.<br />
Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.</p>
<p><strong>Objective = RSS + alpha1 * (sum of square of coefficients) + alpha2 * (sum of absolute value of coefficients)</strong></p>
<p>Elastic-Net to inherit some of Ridge&rsquo;s stability under rotation.<br />
It encourages group effect in case of highly correlated variables.<br />
There are no limitations on the number of selected variables.<br />
It can suffer from double shrinkage.</p>
<h2 id="data-reduction-techniques">Data reduction techniques<a class="headerlink" href="#data-reduction-techniques" title="Permanent link">&para;</a></h2>
<p>Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.<br />
Generally, this is called a data reduction technique.<br />
A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Importing your necessary module</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Feature extraction</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Summarizing components</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Explained Variance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>Explained Variance: [0.88854663 0.06159078 0.02579012]
</code></pre></div>
</td></tr></table>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</code></pre></div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code>[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02
   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]
 [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02
   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]
 [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01
   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]
</code></pre></div>
</td></tr></table>
<p>The transformed dataset (3 principal components) bare little resemblance to the source data.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../exploratory_data_analysis/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Exploratory Data Analysis (EDA)" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Exploratory Data Analysis (EDA)
            </div>
          </div>
        </a>
      
      
        
        <a href="../geospatial_data_in_python/" class="md-footer__link md-footer__link--next" aria-label="Next: Geospatial Data" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Geospatial Data
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>