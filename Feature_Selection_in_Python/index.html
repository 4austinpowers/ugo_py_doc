



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="A Python documentation website.">
      
      
        <link rel="canonical" href="https://ugoproto.github.io/ugo_py_doc/Feature_Selection_in_Python/">
      
      
        <meta name="author" content="Ugo Sparks">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.5">
    
    
      
        <title>Feature Selection - ugo_py_doc</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ffa000">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="amber" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#filter-feature-selection-techniques" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://ugoproto.github.io/ugo_py_doc/" title="ugo_py_doc" class="md-header-nav__button md-logo">
          
            <i class="md-icon">layers</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                ugo_py_doc
              </span>
              <span class="md-header-nav__topic">
                Feature Selection
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/ugoproto/ugo_py_doc.git/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      ugo_py_doc
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://ugoproto.github.io/ugo_py_doc/" title="ugo_py_doc" class="md-nav__button md-logo">
      
        <i class="md-icon">layers</i>
      
    </a>
    ugo_py_doc
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/ugoproto/ugo_py_doc.git/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      ugo_py_doc
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Basics
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Basics
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Py_CS/" title="Python Cheat Sheets" class="md-nav__link">
      Python Cheat Sheets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Databases/" title="Databases" class="md-nav__link">
      Databases
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Datetime/" title="Datetime" class="md-nav__link">
      Datetime
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Decorators/" title="Decorators" class="md-nav__link">
      Decorators
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../exceptions/" title="Exceptions" class="md-nav__link">
      Exceptions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Freeze_the_Code/" title="Freeze the Code" class="md-nav__link">
      Freeze the Code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Gedit_Execute_Highlighted_Python_Code/" title="Gedit, Execute Highlighted Code" class="md-nav__link">
      Gedit, Execute Highlighted Code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Regex/" title="Regular Expressions (regex)" class="md-nav__link">
      Regular Expressions (regex)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Python_Preliminaries/" title="Python Preliminaries" class="md-nav__link">
      Python Preliminaries
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Python_Nice_to_Have/" title="Python Nice to Have" class="md-nav__link">
      Python Nice to Have
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Write_Better_Python/" title="Write Better Python with PEP" class="md-nav__link">
      Write Better Python with PEP
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      Scipy Stack
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Scipy Stack
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../JN_CS/" title="Jupyter Notebook Cheat Sheets" class="md-nav__link">
      Jupyter Notebook Cheat Sheets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Scipy_CS/" title="Scipy Stack Cheat Sheets" class="md-nav__link">
      Scipy Stack Cheat Sheets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../EDA_Machine_Learning_Feature_Engineering_and_Kaggle/" title="EDA, Machine Learning, Feature Engineering, and Kaggle" class="md-nav__link">
      EDA, Machine Learning, Feature Engineering, and Kaggle
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Exploratory Data Analysis/" title="Exploratory Data Analysis (EDA)" class="md-nav__link">
      Exploratory Data Analysis (EDA)
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Feature Selection
      </label>
    
    <a href="./" title="Feature Selection" class="md-nav__link md-nav__link--active">
      Feature Selection
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#filter-feature-selection-techniques" title="Filter feature selection techniques" class="md-nav__link">
    Filter feature selection techniques
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-r" title="In R" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wrapper-feature-selection-techniques" title="Wrapper feature selection techniques" class="md-nav__link">
    Wrapper feature selection techniques
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#another-dataset-another-example" title="Another dataset, another example" class="md-nav__link">
    Another dataset, another example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-r_1" title="In R" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embedded-feature-selection-techniques" title="Embedded feature selection techniques" class="md-nav__link">
    Embedded feature selection techniques
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#takeway" title="Takeway" class="md-nav__link">
    Takeway
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-penalize-the-magnitude-of-coefficients" title="Why Penalize the Magnitude of Coefficients?" class="md-nav__link">
    Why Penalize the Magnitude of Coefficients?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regression" title="Ridge regression" class="md-nav__link">
    Ridge regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regression" title="Lasso regression" class="md-nav__link">
    Lasso regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-difference" title="Key Difference" class="md-nav__link">
    Key Difference
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-use-cases" title="Typical Use Cases" class="md-nav__link">
    Typical Use Cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presence-of-highly-correlated-features" title="Presence of Highly Correlated Features" class="md-nav__link">
    Presence of Highly Correlated Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elasticnet-regression" title="ElasticNet Regression" class="md-nav__link">
    ElasticNet Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-reduction-techniques" title="Data reduction techniques" class="md-nav__link">
    Data reduction techniques
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Geospatial_Data_in_Python/" title="Geospatial Data" class="md-nav__link">
      Geospatial Data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Importing Data into Python/" title="Importing Data" class="md-nav__link">
      Importing Data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Introduction_to_Customer_Segmentation_in_Python/" title="Introduction to Customer Segmentation" class="md-nav__link">
      Introduction to Customer Segmentation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Intro to data.world in Python/" title="Introduction to data.world" class="md-nav__link">
      Introduction to data.world
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../k-NN_Linear_regression_Logit_Scaling_Centering_Noise/" title="k-NN, Linear regression, Logit, Scaling, Centering, Noise" class="md-nav__link">
      k-NN, Linear regression, Logit, Scaling, Centering, Noise
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Overview_of_scikit-learn/" title="Overview of scikit-learn" class="md-nav__link">
      Overview of scikit-learn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Python+And+Excel/" title="Python and Excel" class="md-nav__link">
      Python and Excel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Sentiment_Analysis_with_Twitter/" title="Sentiment Analysis with Twitter" class="md-nav__link">
      Sentiment Analysis with Twitter
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Time_Series_Analysis/" title="Time Series Analysis" class="md-nav__link">
      Time Series Analysis
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Vectors and Arrays (Linear Algebra)/" title="Vectors and Arrays (Linear Algebra)" class="md-nav__link">
      Vectors and Arrays (Linear Algebra)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Viewing+3D+Volumetric+Data+With+Matplotlib/" title="Viewing 3D Volumetric Data with Matplotlib" class="md-nav__link">
      Viewing 3D Volumetric Data with Matplotlib
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Write Idiomatic Pandas Code/" title="Write Idiomatic Pandas Code" class="md-nav__link">
      Write Idiomatic Pandas Code
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Courses
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Courses
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Apprenez a programmer en Python/" title="Apprenez à programmer en Python" class="md-nav__link">
      Apprenez à programmer en Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Automate the Boring Stuff with Python/" title="Automate the Boring Stuff with Python" class="md-nav__link">
      Automate the Boring Stuff with Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Codecademy Python/" title="Codecademy Python" class="md-nav__link">
      Codecademy Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Learn Python the Hard Way/" title="Learn Python the Hard Way" class="md-nav__link">
      Learn Python the Hard Way
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../LPTHW, Python Code Snippets/" title="LPTHW, Python Code Snippets" class="md-nav__link">
      LPTHW, Python Code Snippets
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Manuals
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Manuals
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Managing Your Biological Data with Python/" title="Managing Your Biological Data with Python" class="md-nav__link">
      Managing Your Biological Data with Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Python for Education/" title="Python for Education" class="md-nav__link">
      Python for Education
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#filter-feature-selection-techniques" title="Filter feature selection techniques" class="md-nav__link">
    Filter feature selection techniques
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-r" title="In R" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wrapper-feature-selection-techniques" title="Wrapper feature selection techniques" class="md-nav__link">
    Wrapper feature selection techniques
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#another-dataset-another-example" title="Another dataset, another example" class="md-nav__link">
    Another dataset, another example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-r_1" title="In R" class="md-nav__link">
    In R
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embedded-feature-selection-techniques" title="Embedded feature selection techniques" class="md-nav__link">
    Embedded feature selection techniques
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#takeway" title="Takeway" class="md-nav__link">
    Takeway
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-penalize-the-magnitude-of-coefficients" title="Why Penalize the Magnitude of Coefficients?" class="md-nav__link">
    Why Penalize the Magnitude of Coefficients?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regression" title="Ridge regression" class="md-nav__link">
    Ridge regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regression" title="Lasso regression" class="md-nav__link">
    Lasso regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-difference" title="Key Difference" class="md-nav__link">
    Key Difference
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-use-cases" title="Typical Use Cases" class="md-nav__link">
    Typical Use Cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presence-of-highly-correlated-features" title="Presence of Highly Correlated Features" class="md-nav__link">
    Presence of Highly Correlated Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elasticnet-regression" title="ElasticNet Regression" class="md-nav__link">
    ElasticNet Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-reduction-techniques" title="Data reduction techniques" class="md-nav__link">
    Data reduction techniques
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ugoproto/ugo_py_doc.git/edit/master/docs/Feature_Selection_in_Python.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                  <h1>Feature Selection</h1>
                
                <!--
---

[TOC]
-->

<hr />
<p><strong>Foreword</strong></p>
<p>Notes.</p>
<hr />
<h3 id="a-feature-selection-case">A feature selection case<a class="headerlink" href="#a-feature-selection-case" title="Permanent link">&para;</a></h3>
<p>We use the <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database">Pima Indians Diabetes dataset from Kaggle</a>.<br />
The dataset corresponds to classification tasks on which you need to predict if a person has diabetes based on 8 features.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Loading the primary modules</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Loading the data</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/diabetes.csv&quot;</span><span class="p">)</span>

<span class="c1"># Alternative way</span>
<span class="c1">#url = &quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv&quot;</span>
<span class="c1">#names = [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span>
<span class="c1">#dataframe = pd.read_csv(url, names=names)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Renaming the features, fields or columns AND the response, dependent variable</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;preg&#39;</span><span class="p">,</span> <span class="s1">&#39;plas&#39;</span><span class="p">,</span> <span class="s1">&#39;pres&#39;</span><span class="p">,</span> <span class="s1">&#39;skin&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;pedi&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;class&#39;</span><span class="p">]</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">names</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preg</th>
      <th>plas</th>
      <th>pres</th>
      <th>skin</th>
      <th>test</th>
      <th>mass</th>
      <th>pedi</th>
      <th>age</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Keeping the values only</span>
<span class="c1"># Converting the DataFrame object to a Numpy ndarray</span>
<span class="c1"># to achieve faster computation</span>
<span class="n">array</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Segregating the data into separate variables</span>
<span class="c1"># Features and the labels are separated</span>

<span class="c1"># Features, col 0 to 7</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span>
<span class="c1"># Response, col 8</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:,</span><span class="mi">8</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<h4 id="filter-feature-selection-techniques">Filter feature selection techniques<a class="headerlink" href="#filter-feature-selection-techniques" title="Permanent link">&para;</a></h4>
<p>Let&rsquo;s implement a chi-squared statistical test for non-negative features to select 4 of the best features from the dataset; from the scikit-learn module.  </p>
<p>Other &ldquo;Correlation&rdquo; techniques: Pearsons&rsquo; correlation, LDA, and ANOVA.</p>
<p><strong>A word on the chi-squared test</strong></p>
<p>The chi-squared test is used to determine whether there is a significant difference between the expected frequencies or proportions or distribution and the observed frequencies or proportions or distribution in one or more categories.<br />
It is used to compared the variance of categories or samples vs. population.<br />
It test if each category is mutually exclusive or statistically independent from the other categories.<br />
If the categories are independent, there are not &ldquo;correlated&rdquo;.<br />
The null hypothesis: mutually exclusive or statistically independent. When we reject the null hypothesis, we conclude to statistical dependence or homogeneity.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing the necessary modules</span>
<span class="c1"># SelectKBest class can be used with a suite of different statistical tests</span>
<span class="c1"># to select a specific number of features</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Feature extraction</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Summarizing scores</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>[  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393
   181.304]
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">scores_</span><span class="p">})</span>
</pre></div>
</td></tr></table>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>preg</td>
      <td>111.519691</td>
    </tr>
    <tr>
      <th>1</th>
      <td>plas</td>
      <td>1411.887041</td>
    </tr>
    <tr>
      <th>2</th>
      <td>pres</td>
      <td>17.605373</td>
    </tr>
    <tr>
      <th>3</th>
      <td>skin</td>
      <td>53.108040</td>
    </tr>
    <tr>
      <th>4</th>
      <td>test</td>
      <td>2175.565273</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mass</td>
      <td>127.669343</td>
    </tr>
    <tr>
      <th>6</th>
      <td>pedi</td>
      <td>5.392682</td>
    </tr>
    <tr>
      <th>7</th>
      <td>age</td>
      <td>181.303689</td>
    </tr>
  </tbody>
</table>
</div>

<p>You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): <code class="codehilite">plas, test, mass, age</code>.<br />
This scores will help you further in determining the best features for training your model.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Summarizing selected features (plas, test, mass, age)</span>
<span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:]</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>array([[ 148. ,    0. ,   33.6,   50. ],
       [  85. ,    0. ,   26.6,   31. ],
       [ 183. ,    0. ,   23.3,   32. ],
       [  89. ,   94. ,   28.1,   21. ],
       [ 137. ,  168. ,   43.1,   33. ]])
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;plas&#39;</span><span class="p">,</span><span class="s1">&#39;test&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;age&#39;</span><span class="p">])</span>
</pre></div>
</td></tr></table>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>plas</th>
      <th>test</th>
      <th>mass</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>148.0</td>
      <td>0.0</td>
      <td>33.6</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>85.0</td>
      <td>0.0</td>
      <td>26.6</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>183.0</td>
      <td>0.0</td>
      <td>23.3</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>89.0</td>
      <td>94.0</td>
      <td>28.1</td>
      <td>21.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>137.0</td>
      <td>168.0</td>
      <td>43.1</td>
      <td>33.0</td>
    </tr>
  </tbody>
</table>
</div>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Original dataset</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preg</th>
      <th>plas</th>
      <th>pres</th>
      <th>skin</th>
      <th>test</th>
      <th>mass</th>
      <th>pedi</th>
      <th>age</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="in-r">In R<a class="headerlink" href="#in-r" title="Permanent link">&para;</a></h5>
<p>The caret package provides tools to automatically report on the relevance and importance of attributes in your data and even select the most important features.<br />
Data can contain attributes that are highly correlated with each other.<br />
Many methods perform better if highly correlated attributes are removed.<br />
The caret package provides <code class="codehilite">findCorrelation</code> which will analyze a correlation matrix of your data’s attributes report on attributes that can be removed.<br />
Using the Pima Indians Diabetes dataset, let&rsquo;s remove attributes with an absolute correlation of 0.75 or higher.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Ensuring the results are repeatable</span>
<span class="kp">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="kn">library</span><span class="p">(</span>mlbench<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>caret<span class="p">)</span>

<span class="c1"># Loading the data</span>
data<span class="p">(</span>PimaIndiansDiabetes<span class="p">)</span>

<span class="c1"># Calculating the correlation matrix</span>
correlationMatrix <span class="o">&lt;-</span> cor<span class="p">(</span>PimaIndiansDiabetes<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">])</span>

<span class="c1"># Summarizing the correlation matrix</span>
<span class="kp">print</span><span class="p">(</span>correlationMatrix<span class="p">)</span>

<span class="c1"># Finding attributes that are highly corrected (ideally &gt;0.75)</span>
highlyCorrelated <span class="o">&lt;-</span> findCorrelation<span class="p">(</span>correlationMatrix<span class="p">,</span> cutoff<span class="o">=</span><span class="m">0.5</span><span class="p">)</span>

<span class="c1"># Printing the indexes of highly correlated attributes</span>
<span class="kp">print</span><span class="p">(</span>highlyCorrelated<span class="p">)</span>r
</pre></div>
</td></tr></table>

<p>The importance of features can be estimated from data by building a model.<br />
Some methods like decision trees have a built-in mechanism to report on variable importance.<br />
For other algorithms, the importance can be estimated using a ROC curve analysis conducted for each attribute.<br />
Let&rsquo;s constructs a Learning Vector Quantization (LVQ) model.  </p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Ensuring the results are repeatable</span>
<span class="kp">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="kn">library</span><span class="p">(</span>mlbench<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>caret<span class="p">)</span>

<span class="c1"># Loading the data</span>
data<span class="p">(</span>PimaIndiansDiabetes<span class="p">)</span>

<span class="c1"># Calculating the correlation matrix</span>
correlationMatrix <span class="o">&lt;-</span> cor<span class="p">(</span>PimaIndiansDiabetes<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">])</span>

<span class="c1"># Summarizing the correlation matrix</span>
<span class="kp">print</span><span class="p">(</span>correlationMatrix<span class="p">)</span>

<span class="c1"># Finding attributes that are highly corrected (ideally &gt; 0.75)</span>
highlyCorrelated <span class="o">&lt;-</span> findCorrelation<span class="p">(</span>correlationMatrix<span class="p">,</span> cutoff<span class="o">=</span><span class="m">0.5</span><span class="p">)</span>

<span class="c1"># Printing indexes of highly correlated attributes</span>
<span class="kp">print</span><span class="p">(</span>highlyCorrelated<span class="p">)</span>
</pre></div>
</td></tr></table>

<h4 id="wrapper-feature-selection-techniques">Wrapper feature selection techniques<a class="headerlink" href="#wrapper-feature-selection-techniques" title="Permanent link">&para;</a></h4>
<p>Let&rsquo;s implement a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE">Recursive Feature Elimination</a> from the scikit-learn module.</p>
<p>Other techniques: Forward Selection, Backward Elimination, and Combination of forward selection and backward elimination.</p>
<p>The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.<br />
It uses the model accuracy to identify which attributes (and a combination of attributes) contribute the most to predicting the target attribute.<br />
You use RFE with the Logistic Regression classifier to select the top 3 features.<br />
The choice of algorithms does not matter too much as long as it is consistent.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Feature extraction</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Summarizing the selection of the attributes</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Selected Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Feature Ranking: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Num Features: 3
Selected Features: [ True False False False False  True  True False]
Feature Ranking: [1 2 3 5 6 1 1 4]


/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;selected&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">})</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>ranking</th>
      <th>selected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>preg</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>plas</td>
      <td>2</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>pres</td>
      <td>3</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>skin</td>
      <td>5</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>test</td>
      <td>6</td>
      <td>False</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mass</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>pedi</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>7</th>
      <td>age</td>
      <td>4</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<p>You can see that RFE chose the top 3 features as <code class="codehilite">preg, mass, pedi</code>.<br />
These are marked True in the support array and marked with a choice “1” in the ranking array.</p>
<p>You can also use RFE with the Bagged decision trees like Random Forest and Extra Trees to estimate the importance of features.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Feature extraction</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Summarizing the selection of the attributes</span>
<span class="k">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>[0.10490793 0.20041541 0.0958495  0.08070628 0.0849718  0.15844774
 0.12226854 0.15243279]


/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Features Importance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Num Features: 8
Features Importance: [0.10490793 0.20041541 0.0958495  0.08070628 0.0849718  0.15844774
 0.12226854 0.15243279]
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">})</span><span class="o">.</span>\
    <span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
      <th>names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.200415</td>
      <td>plas</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.158448</td>
      <td>mass</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.152433</td>
      <td>age</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.122269</td>
      <td>pedi</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.104908</td>
      <td>preg</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.095849</td>
      <td>pres</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.084972</td>
      <td>test</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.080706</td>
      <td>skin</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="another-dataset-another-example">Another dataset, another example<a class="headerlink" href="#another-dataset-another-example" title="Permanent link">&para;</a></h5>
<p>Let&rsquo;s tackle another example using the built-in iris dataset, reusing the Logistic Regression and the Extra Tree Ensemble.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Loading the iris datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span><span class="s1">&#39;sepal width&#39;</span><span class="p">,</span><span class="s1">&#39;petal length&#39;</span><span class="p">,</span><span class="s1">&#39;petal width&#39;</span><span class="p">]</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<span class="n">dataset_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Creating a base classifier used to evaluate a subset of attributes</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Creating the RFE model and select 3 attributes</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to &#39;auto&#39; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Selected Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Feature Ranking: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Num Features: 3
Selected Features: [False  True  True  True]
Feature Ranking: [2 1 1 1]
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">,</span> <span class="s1">&#39;selected&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">:</span> <span class="n">fit</span><span class="o">.</span><span class="n">ranking_</span><span class="p">})</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>ranking</th>
      <th>selected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>sepal length</td>
      <td>2</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>sepal width</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>petal length</td>
      <td>1</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>petal width</td>
      <td>1</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>

<p>Keep top-ranking features (rank 1) and leave out the other features (rank 2).</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing your necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Creating a base classifier</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>

<span class="c1"># Creating the RFE model and select 3 attributes</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">[0.08470961 0.02095061 0.37336503 0.52097475]</span>


<span class="na">/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.</span>
  <span class="na">&quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Num Features: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Features Importance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rfe</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Num Features: 4
Features Importance: [0.08470961 0.02095061 0.37336503 0.52097475]
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Alternative view</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;names&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">rfe</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">})</span><span class="o">.</span>\
    <span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
      <th>names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>0.520975</td>
      <td>petal width</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.373365</td>
      <td>petal length</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.084710</td>
      <td>sepal length</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.020951</td>
      <td>sepal width</td>
    </tr>
  </tbody>
</table>
</div>

<p>The last results confirm the previous results.</p>
<h5 id="in-r_1">In R<a class="headerlink" href="#in-r_1" title="Permanent link">&para;</a></h5>
<p>Recursive Feature Elimination or RFE.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Ensuring the results are repeatable</span>
<span class="kp">set.seed</span><span class="p">(</span><span class="m">7</span><span class="p">)</span>

<span class="c1"># Loading the packages</span>
<span class="kn">library</span><span class="p">(</span>mlbench<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>caret<span class="p">)</span>

<span class="c1"># Loading the data</span>
data<span class="p">(</span>PimaIndiansDiabetes<span class="p">)</span>

<span class="c1"># Defining the control using a random forest selection function</span>
control <span class="o">&lt;-</span> rfeControl<span class="p">(</span>functions<span class="o">=</span>rfFuncs<span class="p">,</span> method<span class="o">=</span><span class="s">&quot;cv&quot;</span><span class="p">,</span> number<span class="o">=</span><span class="m">10</span><span class="p">)</span>

<span class="c1"># Running the RFE algorithm</span>
results <span class="o">&lt;-</span> rfe<span class="p">(</span>PimaIndiansDiabetes<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">],</span> PimaIndiansDiabetes<span class="p">[,</span><span class="m">9</span><span class="p">],</span> sizes<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">),</span> rfeControl<span class="o">=</span>control<span class="p">)</span>

<span class="c1"># Summarize the results</span>
<span class="kp">print</span><span class="p">(</span>results<span class="p">)</span>

<span class="c1"># Listing the chosen features</span>
predictors<span class="p">(</span>results<span class="p">)</span>

<span class="c1"># Plotting the results</span>
plot<span class="p">(</span>results<span class="p">,</span> type<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;g&quot;</span><span class="p">,</span> <span class="s">&quot;o&quot;</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<h4 id="embedded-feature-selection-techniques">Embedded feature selection techniques<a class="headerlink" href="#embedded-feature-selection-techniques" title="Permanent link">&para;</a></h4>
<p>Let&rsquo;s use the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge regression</a> from the scikit-learn module; a regularization technique as well.</p>
<p>Other techniques: LASSO and Elastic Net.</p>
<p>Find out more about <a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#thre">regularization techniques</a>.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing your necessary module</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Using Ridge regression to determine the R-squared</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)
</pre></div>
</td></tr></table>

<p>In order to better understand the results of Ridge regression, you will implement a little helper function that will help you to print the results in a better so that you can interpret them easily.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Implementing a function for pretty-printing the coefficients</span>
<span class="k">def</span> <span class="nf">pretty_print_coefs</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">names</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">sort</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">names</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">))]</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sort</span><span class="p">:</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span>  <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">return</span> <span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> * </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">name</span><span class="p">)</span>
                                   <span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Applying the function</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="c1"># Applying the function with the names</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">names</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7

Ridge model: 0.021 * preg + 0.006 * plas + -0.002 * pres + 0.0 * skin + -0.0 * test + 0.013 * mass + 0.145 * pedi + 0.003 * age
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Applying the function with the names and sorting the results</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Ridge model:&quot;</span><span class="p">,</span> <span class="n">pretty_print_coefs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="bp">True</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Ridge model: 0.145 * pedi + 0.021 * preg + 0.013 * mass + 0.006 * plas + 0.003 * age + -0.002 * pres + -0.0 * test + 0.0 * skin
</pre></div>
</td></tr></table>

<p>You can spot all the coefficient terms appended with the feature variables.<br />
You can pick the most essential features.<br />
The sorted top 3 features are <code class="codehilite">pedi, preg, mass</code>.  </p>
<p><strong>A word on the Ridge regression</strong></p>
<ul>
<li>It is also known as L2-Regularization.</li>
<li>For correlated features, it means that they tend to get similar coefficients.</li>
<li>Feature having negative coefficients don&rsquo;t contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision-making process.</li>
</ul>
<h4 id="takeway">Takeway<a class="headerlink" href="#takeway" title="Permanent link">&para;</a></h4>
<p>The three techniques help to understand the features of a particular dataset in a comprehensive manner.<br />
Feature selection is essentially a part of <strong>data preprocessing</strong> which is considered to be the most time-consuming part of any machine learning pipeline.<br />
These techniques will help you to approach it in a more systematic way and machine learning friendly way.<br />
You will be able to interpret the features more accurately.</p>
<h3 id="why-penalize-the-magnitude-of-coefficients">Why Penalize the Magnitude of Coefficients?<a class="headerlink" href="#why-penalize-the-magnitude-of-coefficients" title="Permanent link">&para;</a></h3>
<p>Given a sine curve (between 60° and 300°) and some random noise using the following code:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing modules</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">matplotlib.pylab</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">10</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Defining input array with angles</span>
<span class="c1"># from 60deg to 300deg converted to radians</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">4</span><span class="p">)])</span>

<span class="c1"># Setting seed for reproducability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]),</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">);</span>
</pre></div>
</td></tr></table>

<p><img alt="" src="../img/Feature_Selection_in_Python/output_45_0.png" /></p>
<p>Let&rsquo;s try to estimate the sine function using polynomial regression with powers of x form 1 to 15.<br />
Let&rsquo;s add a column for each power up to 15.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Power of 1 is already there</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">):</span>  
    <span class="n">colname</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="c1"># new var will be x_power</span>
    <span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">**</span><span class="n">i</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>x_2</th>
      <th>x_3</th>
      <th>x_4</th>
      <th>x_5</th>
      <th>x_6</th>
      <th>x_7</th>
      <th>x_8</th>
      <th>x_9</th>
      <th>x_10</th>
      <th>x_11</th>
      <th>x_12</th>
      <th>x_13</th>
      <th>x_14</th>
      <th>x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.047198</td>
      <td>1.065763</td>
      <td>1.096623</td>
      <td>1.148381</td>
      <td>1.202581</td>
      <td>1.259340</td>
      <td>1.318778</td>
      <td>1.381021</td>
      <td>1.446202</td>
      <td>1.514459</td>
      <td>1.585938</td>
      <td>1.660790</td>
      <td>1.739176</td>
      <td>1.821260</td>
      <td>1.907219</td>
      <td>1.997235</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.117011</td>
      <td>1.006086</td>
      <td>1.247713</td>
      <td>1.393709</td>
      <td>1.556788</td>
      <td>1.738948</td>
      <td>1.942424</td>
      <td>2.169709</td>
      <td>2.423588</td>
      <td>2.707173</td>
      <td>3.023942</td>
      <td>3.377775</td>
      <td>3.773011</td>
      <td>4.214494</td>
      <td>4.707635</td>
      <td>5.258479</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.186824</td>
      <td>0.695374</td>
      <td>1.408551</td>
      <td>1.671702</td>
      <td>1.984016</td>
      <td>2.354677</td>
      <td>2.794587</td>
      <td>3.316683</td>
      <td>3.936319</td>
      <td>4.671717</td>
      <td>5.544505</td>
      <td>6.580351</td>
      <td>7.809718</td>
      <td>9.268760</td>
      <td>11.000386</td>
      <td>13.055521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.256637</td>
      <td>0.949799</td>
      <td>1.579137</td>
      <td>1.984402</td>
      <td>2.493673</td>
      <td>3.133642</td>
      <td>3.937850</td>
      <td>4.948448</td>
      <td>6.218404</td>
      <td>7.814277</td>
      <td>9.819710</td>
      <td>12.339811</td>
      <td>15.506664</td>
      <td>19.486248</td>
      <td>24.487142</td>
      <td>30.771450</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.326450</td>
      <td>1.063496</td>
      <td>1.759470</td>
      <td>2.333850</td>
      <td>3.095735</td>
      <td>4.106339</td>
      <td>5.446854</td>
      <td>7.224981</td>
      <td>9.583578</td>
      <td>12.712139</td>
      <td>16.862020</td>
      <td>22.366630</td>
      <td>29.668222</td>
      <td>39.353420</td>
      <td>52.200353</td>
      <td>69.241170</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now that we have all the 15 powers, let&rsquo;s make 15 different linear regression models with each model containing variables with powers of x from 1 to the particular model number.<br />
For example, the feature set of model 8 will be {x, x_2, x_3, &hellip; , x_8}.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing the Linear Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="p">):</span>
    <span class="c1">#initialize predictors:</span>
    <span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">power</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">power</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>

    <span class="c1"># Fitting the model</span>
    <span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1"># Checking if a plot is to be made for the entered power</span>
    <span class="k">if</span> <span class="n">power</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">power</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for power: </span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">power</span><span class="p">)</span>

    <span class="c1"># Returning the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</pre></div>
</td></tr></table>

<p>Now, we can make all 15 models and compare the results.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Initializing a DataFrame to store the results</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;model_pow_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">coef_matrix_simple</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Defining the powers for which a plot is required</span>
<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span><span class="mi">3</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span><span class="mi">6</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span><span class="mi">9</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span><span class="mi">12</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span><span class="mi">15</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Iterating through all powers and assimilate results</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">coef_matrix_simple</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="n">models_to_plot</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p><img alt="" src="../img/Feature_Selection_in_Python/output_54_0.png" /></p>
<p>As the model complexity increases, the models tend to fit even smaller deviations in the training data set.  </p>
<p>This leads to overfitting.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;{:,.2g}&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_simple</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>model_pow_1</th>
      <td>3.3</td>
      <td>2</td>
      <td>-0.62</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_2</th>
      <td>3.3</td>
      <td>1.9</td>
      <td>-0.58</td>
      <td>-0.006</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_3</th>
      <td>1.1</td>
      <td>-1.1</td>
      <td>3</td>
      <td>-1.3</td>
      <td>0.14</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_4</th>
      <td>1.1</td>
      <td>-0.27</td>
      <td>1.7</td>
      <td>-0.53</td>
      <td>-0.036</td>
      <td>0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_5</th>
      <td>1</td>
      <td>3</td>
      <td>-5.1</td>
      <td>4.7</td>
      <td>-1.9</td>
      <td>0.33</td>
      <td>-0.021</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_6</th>
      <td>0.99</td>
      <td>-2.8</td>
      <td>9.5</td>
      <td>-9.7</td>
      <td>5.2</td>
      <td>-1.6</td>
      <td>0.23</td>
      <td>-0.014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_7</th>
      <td>0.93</td>
      <td>19</td>
      <td>-56</td>
      <td>69</td>
      <td>-45</td>
      <td>17</td>
      <td>-3.5</td>
      <td>0.4</td>
      <td>-0.019</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_8</th>
      <td>0.92</td>
      <td>43</td>
      <td>-1.4e+02</td>
      <td>1.8e+02</td>
      <td>-1.3e+02</td>
      <td>58</td>
      <td>-15</td>
      <td>2.4</td>
      <td>-0.21</td>
      <td>0.0077</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_9</th>
      <td>0.87</td>
      <td>1.7e+02</td>
      <td>-6.1e+02</td>
      <td>9.6e+02</td>
      <td>-8.5e+02</td>
      <td>4.6e+02</td>
      <td>-1.6e+02</td>
      <td>37</td>
      <td>-5.2</td>
      <td>0.42</td>
      <td>-0.015</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_10</th>
      <td>0.87</td>
      <td>1.4e+02</td>
      <td>-4.9e+02</td>
      <td>7.3e+02</td>
      <td>-6e+02</td>
      <td>2.9e+02</td>
      <td>-87</td>
      <td>15</td>
      <td>-0.81</td>
      <td>-0.14</td>
      <td>0.026</td>
      <td>-0.0013</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_11</th>
      <td>0.87</td>
      <td>-75</td>
      <td>5.1e+02</td>
      <td>-1.3e+03</td>
      <td>1.9e+03</td>
      <td>-1.6e+03</td>
      <td>9.1e+02</td>
      <td>-3.5e+02</td>
      <td>91</td>
      <td>-16</td>
      <td>1.8</td>
      <td>-0.12</td>
      <td>0.0034</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_12</th>
      <td>0.87</td>
      <td>-3.4e+02</td>
      <td>1.9e+03</td>
      <td>-4.4e+03</td>
      <td>6e+03</td>
      <td>-5.2e+03</td>
      <td>3.1e+03</td>
      <td>-1.3e+03</td>
      <td>3.8e+02</td>
      <td>-80</td>
      <td>12</td>
      <td>-1.1</td>
      <td>0.062</td>
      <td>-0.0016</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_13</th>
      <td>0.86</td>
      <td>3.2e+03</td>
      <td>-1.8e+04</td>
      <td>4.5e+04</td>
      <td>-6.7e+04</td>
      <td>6.6e+04</td>
      <td>-4.6e+04</td>
      <td>2.3e+04</td>
      <td>-8.5e+03</td>
      <td>2.3e+03</td>
      <td>-4.5e+02</td>
      <td>62</td>
      <td>-5.7</td>
      <td>0.31</td>
      <td>-0.0078</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_14</th>
      <td>0.79</td>
      <td>2.4e+04</td>
      <td>-1.4e+05</td>
      <td>3.8e+05</td>
      <td>-6.1e+05</td>
      <td>6.6e+05</td>
      <td>-5e+05</td>
      <td>2.8e+05</td>
      <td>-1.2e+05</td>
      <td>3.7e+04</td>
      <td>-8.5e+03</td>
      <td>1.5e+03</td>
      <td>-1.8e+02</td>
      <td>15</td>
      <td>-0.73</td>
      <td>0.017</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>model_pow_15</th>
      <td>0.7</td>
      <td>-3.6e+04</td>
      <td>2.4e+05</td>
      <td>-7.5e+05</td>
      <td>1.4e+06</td>
      <td>-1.7e+06</td>
      <td>1.5e+06</td>
      <td>-1e+06</td>
      <td>5e+05</td>
      <td>-1.9e+05</td>
      <td>5.4e+04</td>
      <td>-1.2e+04</td>
      <td>1.9e+03</td>
      <td>-2.2e+02</td>
      <td>17</td>
      <td>-0.81</td>
      <td>0.018</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is clearly evident that the size of coefficients increase exponentially with increase in model complexity.<br />
It means that we&rsquo;re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome.<br />
When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data.</p>
<h4 id="ridge-regression">Ridge regression<a class="headerlink" href="#ridge-regression" title="Permanent link">&para;</a></h4>
<p>A ridge regression performs L2 regularization&rsquo;, i.e. it adds a factor of sum of squares of coefficients in the optimization objective.</p>
<p><strong>Objective = RSS + alpha * (sum of square of coefficients)</strong></p>
<p>The alpha parameter balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients.</p>
<ul>
<li>alpha = 0, a simple linear regression;</li>
<li>alpha = infinite, infinite weight on square of coefficients, anything less than zero will make the objective infinite; all coefficients zero;</li>
<li>0 &lt; alpha &lt; infinite, somewhere between 0 and 1 for simple linear regression.</li>
</ul>
<p>One thing is for sure that any non-zero value would give values less than that of simple linear regression.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing the Ridge Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">ridge_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="p">{}):</span>
    <span class="c1"># Fitting the model</span>
    <span class="n">ridgereg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ridgereg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridgereg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1"># Checking if a plot is to be made for the entered alpha</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">alpha</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for alpha: </span><span class="si">%.3g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha</span><span class="p">)</span>

    <span class="c1"># Returning the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ridgereg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ridgereg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</pre></div>
</td></tr></table>

<p>Let&rsquo;s analyze the result of Ridge regression for 10 different values of alpha ranging from 1e-15 to 20.<br />
Each of these 10 models will contain all the 15 variables and only the value of alpha would differ.<br />
This is different from the simple linear regression case where each model had a subset of features.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Initializing predictors to be set of 15 powers of x</span>
<span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)])</span>

<span class="c1"># Setting the different values of alpha to be tested</span>
<span class="n">alpha_ridge</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-15</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Initializing the dataframe for storing coefficients.</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha_</span><span class="si">%.2g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha_ridge</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">coef_matrix_ridge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1e-15</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">coef_matrix_ridge</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,]</span> <span class="o">=</span> <span class="n">ridge_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha_ridge</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">models_to_plot</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:125: LinAlgWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number4.572933e-17
  overwrite_a=True).T
</pre></div>
</td></tr></table>

<p><img alt="" src="../img/Feature_Selection_in_Python/output_61_1.png" /></p>
<p>As the value of alpha increases, the model complexity reduces.<br />
Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (alpha = 5, for example).<br />
Thus alpha should be chosen wisely.<br />
A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;{:,.2g}&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_ridge</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha_1e-15</th>
      <td>0.87</td>
      <td>94</td>
      <td>-3e+02</td>
      <td>3.7e+02</td>
      <td>-2.3e+02</td>
      <td>61</td>
      <td>2.6</td>
      <td>-5.1</td>
      <td>0.62</td>
      <td>0.17</td>
      <td>-0.03</td>
      <td>-0.0052</td>
      <td>0.001</td>
      <td>0.00017</td>
      <td>-4.6e-05</td>
      <td>3.1e-06</td>
      <td>-3.8e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-10</th>
      <td>0.92</td>
      <td>11</td>
      <td>-29</td>
      <td>31</td>
      <td>-15</td>
      <td>2.9</td>
      <td>0.17</td>
      <td>-0.091</td>
      <td>-0.011</td>
      <td>0.002</td>
      <td>0.00064</td>
      <td>2.4e-05</td>
      <td>-2e-05</td>
      <td>-4.2e-06</td>
      <td>2.2e-07</td>
      <td>2.3e-07</td>
      <td>-2.3e-08</td>
    </tr>
    <tr>
      <th>alpha_1e-08</th>
      <td>0.95</td>
      <td>1.3</td>
      <td>-1.5</td>
      <td>1.7</td>
      <td>-0.68</td>
      <td>0.039</td>
      <td>0.016</td>
      <td>0.00016</td>
      <td>-0.00036</td>
      <td>-5.4e-05</td>
      <td>-2.9e-07</td>
      <td>1.1e-06</td>
      <td>1.9e-07</td>
      <td>2e-08</td>
      <td>3.9e-09</td>
      <td>8.2e-10</td>
      <td>-4.6e-10</td>
    </tr>
    <tr>
      <th>alpha_0.0001</th>
      <td>0.96</td>
      <td>0.56</td>
      <td>0.55</td>
      <td>-0.13</td>
      <td>-0.026</td>
      <td>-0.0028</td>
      <td>-0.00011</td>
      <td>4.1e-05</td>
      <td>1.5e-05</td>
      <td>3.7e-06</td>
      <td>7.4e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.9e-09</td>
      <td>-1.3e-10</td>
      <td>-1.5e-10</td>
      <td>-6.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.001</th>
      <td>1</td>
      <td>0.82</td>
      <td>0.31</td>
      <td>-0.087</td>
      <td>-0.02</td>
      <td>-0.0028</td>
      <td>-0.00022</td>
      <td>1.8e-05</td>
      <td>1.2e-05</td>
      <td>3.4e-06</td>
      <td>7.3e-07</td>
      <td>1.3e-07</td>
      <td>1.9e-08</td>
      <td>1.7e-09</td>
      <td>-1.5e-10</td>
      <td>-1.4e-10</td>
      <td>-5.2e-11</td>
    </tr>
    <tr>
      <th>alpha_0.01</th>
      <td>1.4</td>
      <td>1.3</td>
      <td>-0.088</td>
      <td>-0.052</td>
      <td>-0.01</td>
      <td>-0.0014</td>
      <td>-0.00013</td>
      <td>7.2e-07</td>
      <td>4.1e-06</td>
      <td>1.3e-06</td>
      <td>3e-07</td>
      <td>5.6e-08</td>
      <td>9e-09</td>
      <td>1.1e-09</td>
      <td>4.3e-11</td>
      <td>-3.1e-11</td>
      <td>-1.5e-11</td>
    </tr>
    <tr>
      <th>alpha_1</th>
      <td>5.6</td>
      <td>0.97</td>
      <td>-0.14</td>
      <td>-0.019</td>
      <td>-0.003</td>
      <td>-0.00047</td>
      <td>-7e-05</td>
      <td>-9.9e-06</td>
      <td>-1.3e-06</td>
      <td>-1.4e-07</td>
      <td>-9.3e-09</td>
      <td>1.3e-09</td>
      <td>7.8e-10</td>
      <td>2.4e-10</td>
      <td>6.2e-11</td>
      <td>1.4e-11</td>
      <td>3.2e-12</td>
    </tr>
    <tr>
      <th>alpha_5</th>
      <td>14</td>
      <td>0.55</td>
      <td>-0.059</td>
      <td>-0.0085</td>
      <td>-0.0014</td>
      <td>-0.00024</td>
      <td>-4.1e-05</td>
      <td>-6.9e-06</td>
      <td>-1.1e-06</td>
      <td>-1.9e-07</td>
      <td>-3.1e-08</td>
      <td>-5.1e-09</td>
      <td>-8.2e-10</td>
      <td>-1.3e-10</td>
      <td>-2e-11</td>
      <td>-3e-12</td>
      <td>-4.2e-13</td>
    </tr>
    <tr>
      <th>alpha_10</th>
      <td>18</td>
      <td>0.4</td>
      <td>-0.037</td>
      <td>-0.0055</td>
      <td>-0.00095</td>
      <td>-0.00017</td>
      <td>-3e-05</td>
      <td>-5.2e-06</td>
      <td>-9.2e-07</td>
      <td>-1.6e-07</td>
      <td>-2.9e-08</td>
      <td>-5.1e-09</td>
      <td>-9.1e-10</td>
      <td>-1.6e-10</td>
      <td>-2.9e-11</td>
      <td>-5.1e-12</td>
      <td>-9.1e-13</td>
    </tr>
    <tr>
      <th>alpha_20</th>
      <td>23</td>
      <td>0.28</td>
      <td>-0.022</td>
      <td>-0.0034</td>
      <td>-0.0006</td>
      <td>-0.00011</td>
      <td>-2e-05</td>
      <td>-3.6e-06</td>
      <td>-6.6e-07</td>
      <td>-1.2e-07</td>
      <td>-2.2e-08</td>
      <td>-4e-09</td>
      <td>-7.5e-10</td>
      <td>-1.4e-10</td>
      <td>-2.5e-11</td>
      <td>-4.7e-12</td>
      <td>-8.7e-13</td>
    </tr>
  </tbody>
</table>
</div>

<p>The RSS increases with increases in alpha, this model complexity reduces.<br />
An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients.<br />
High alpha values can lead to significant underfitting.<br />
Note the rapid increase in RSS for values of alpha greater than 1.<br />
Though the coefficients are very very small, they are NOT zero.<br />
Let’s reconfirm the same by determining the number of zeros in each row of the coefficients dataset.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">coef_matrix_ridge</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>alpha_1e-15     0
alpha_1e-10     0
alpha_1e-08     0
alpha_0.0001    0
alpha_0.001     0
alpha_0.01      0
alpha_1         0
alpha_5         0
alpha_10        0
alpha_20        0
dtype: int64
</pre></div>
</td></tr></table>

<h4 id="lasso-regression">Lasso regression<a class="headerlink" href="#lasso-regression" title="Permanent link">&para;</a></h4>
<p>LASSO stands for <em>Least Absolute Shrinkage and Selection Operator</em>.<br />
Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective.  </p>
<p><strong>Objective = RSS + alpha * (sum of absolute value of coefficients)</strong></p>
<p>The alpha works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients.</p>
<ul>
<li>alpha = 0, a simple linear regression;</li>
<li>alpha = infinite, infinite weight on square of coefficients, anything less than zero will make the objective infinite; all coefficients zero.</li>
<li>0 &lt; alpha &lt; infinite, somewhere between 0 and 1 for simple linear regression.</li>
</ul>
<p>It appears to be very similar to the Ridge regression.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing the Ridge Regression model from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">lasso_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">models_to_plot</span><span class="o">=</span><span class="p">{}):</span>
    <span class="c1">#Fit the model</span>
    <span class="n">lassoreg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>
    <span class="n">lassoreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lassoreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>

    <span class="c1">#Check if a plot is to be made for the entered alpha</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">models_to_plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">models_to_plot</span><span class="p">[</span><span class="n">alpha</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Plot for alpha: </span><span class="si">%.3g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha</span><span class="p">)</span>

    <span class="c1">#Return the result in pre-defined format</span>
    <span class="n">rss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">rss</span><span class="p">]</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">lassoreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">])</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lassoreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>
</pre></div>
</td></tr></table>

<p>The additional parameters defined in Lasso function <code class="codehilite">max_iter</code> is the maximum number of iterations for which we want the model to run if it doesn&rsquo;t converge before.<br />
This exists for Ridge regression as well but setting this to a higher than default value was required in this case.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Initializing predictors to all 15 powers of x</span>
<span class="n">predictors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">predictors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)])</span>

<span class="c1"># Defining the alpha values to test</span>
<span class="n">alpha_lasso</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-15</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="c1"># Initializing the dataframe to store coefficients</span>
<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rss&#39;</span><span class="p">,</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;coef_x_</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)]</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha_</span><span class="si">%.2g</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">alpha_lasso</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">coef_matrix_lasso</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="c1"># Defining the models to plot</span>
<span class="n">models_to_plot</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1e-10</span><span class="p">:</span><span class="mi">231</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">:</span><span class="mi">232</span><span class="p">,</span><span class="mf">1e-4</span><span class="p">:</span><span class="mi">233</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">:</span><span class="mi">234</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">:</span><span class="mi">235</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">236</span><span class="p">}</span>

<span class="c1"># Iterating over the 10 alpha values:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">coef_matrix_lasso</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,]</span> <span class="o">=</span> <span class="n">lasso_regression</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">alpha_lasso</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">models_to_plot</span><span class="p">);</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
/home/ugo/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
  ConvergenceWarning)
</pre></div>
</td></tr></table>

<p><img alt="" src="../img/Feature_Selection_in_Python/output_70_1.png" /></p>
<p>This again tells us that the model complexity decreases with increases in the values of alpha.<br />
But notice the straight line at alpha=1.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Displaying the analysis</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;{:,.2g}&#39;</span><span class="o">.</span><span class="n">format</span>
<span class="n">coef_matrix_lasso</span>
</pre></div>
</td></tr></table>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rss</th>
      <th>intercept</th>
      <th>coef_x_1</th>
      <th>coef_x_2</th>
      <th>coef_x_3</th>
      <th>coef_x_4</th>
      <th>coef_x_5</th>
      <th>coef_x_6</th>
      <th>coef_x_7</th>
      <th>coef_x_8</th>
      <th>coef_x_9</th>
      <th>coef_x_10</th>
      <th>coef_x_11</th>
      <th>coef_x_12</th>
      <th>coef_x_13</th>
      <th>coef_x_14</th>
      <th>coef_x_15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha_1e-15</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00089</td>
      <td>0.0016</td>
      <td>-0.00012</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.4e-09</td>
      <td>1.8e-10</td>
      <td>-2e-10</td>
      <td>-9.2e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-10</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00088</td>
      <td>0.0016</td>
      <td>-0.00012</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.4e-09</td>
      <td>1.8e-10</td>
      <td>-2e-10</td>
      <td>-9.2e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-08</th>
      <td>0.96</td>
      <td>0.22</td>
      <td>1.1</td>
      <td>-0.37</td>
      <td>0.00077</td>
      <td>0.0016</td>
      <td>-0.00011</td>
      <td>-6.4e-05</td>
      <td>-6.3e-06</td>
      <td>1.4e-06</td>
      <td>7.8e-07</td>
      <td>2.1e-07</td>
      <td>4e-08</td>
      <td>5.3e-09</td>
      <td>2e-10</td>
      <td>-1.9e-10</td>
      <td>-9.3e-11</td>
    </tr>
    <tr>
      <th>alpha_1e-05</th>
      <td>0.96</td>
      <td>0.5</td>
      <td>0.6</td>
      <td>-0.13</td>
      <td>-0.038</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7e-06</td>
      <td>1e-06</td>
      <td>7.7e-08</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0</td>
      <td>-7e-11</td>
    </tr>
    <tr>
      <th>alpha_0.0001</th>
      <td>1</td>
      <td>0.9</td>
      <td>0.17</td>
      <td>-0</td>
      <td>-0.048</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>9.5e-06</td>
      <td>5.1e-07</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-4.4e-11</td>
    </tr>
    <tr>
      <th>alpha_0.001</th>
      <td>1.7</td>
      <td>1.3</td>
      <td>-0</td>
      <td>-0.13</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1.5e-08</td>
      <td>7.5e-10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>alpha_0.01</th>
      <td>3.6</td>
      <td>1.8</td>
      <td>-0.55</td>
      <td>-0.00056</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>alpha_1</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
    <tr>
      <th>alpha_5</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
    <tr>
      <th>alpha_10</th>
      <td>37</td>
      <td>0.038</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
      <td>-0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Apart from the expected inference of higher RSS for higher alphas, we can see the following:</p>
<ul>
<li>For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables);</li>
<li>For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression;</li>
<li>Many of the coefficients are zero even for very small values of alpha.</li>
</ul>
<p>The real difference from ridge is coming out in the last inference.<br />
Let&rsquo;s check the number of coefficients which are zero in each model using the following code.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">coef_matrix_lasso</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>alpha_1e-15      0
alpha_1e-10      0
alpha_1e-08      0
alpha_1e-05      8
alpha_0.0001    10
alpha_0.001     12
alpha_0.01      13
alpha_1         15
alpha_5         15
alpha_10        15
dtype: int64
</pre></div>
</td></tr></table>

<p>For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables).<br />
For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression.<br />
Many of the coefficients are zero even for very small values of alpha.<br />
The real difference from ridge is coming out in the last inference.<br />
We can observe that even for a small value of alpha, a significant number of coefficients are zero.<br />
This also explains the horizontal line fit for alpha=1 in the lasso plots, it&rsquo;s just a baseline model!<br />
This phenomenon of most of the coefficients being zero is called &lsquo;sparsity&rsquo;. Although lasso performs feature selection, this level of sparsity is achieved in special cases only which we&rsquo;ll discuss towards the end.</p>
<h4 id="key-difference">Key Difference<a class="headerlink" href="#key-difference" title="Permanent link">&para;</a></h4>
<ul>
<li>Ridge: it includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.</li>
<li>Lasso: along with shrinking coefficients, lasso performs feature selection as well. (Remember the &lsquo;selection&rsquo; in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.</li>
</ul>
<p>Traditionally, techniques like stepwise regression were used to perform feature selection and make parsimonious models.<br />
But with advancements in Machine Learning, ridge and lasso regression provide very good alternatives as they give much better output, require fewer tuning parameters and can be automated to a large extent.</p>
<h4 id="typical-use-cases">Typical Use Cases<a class="headerlink" href="#typical-use-cases" title="Permanent link">&para;</a></h4>
<ul>
<li>Ridge: it is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of an exorbitantly high number of features, say in millions, as it will pose computational challenges.</li>
<li>Lasso: since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the number of features is in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.</li>
</ul>
<p>It&rsquo;s not hard to see why the stepwise selection techniques become practically very cumbersome to implement in high dimensionality cases.<br />
Thus, lasso provides a significant advantage.</p>
<h4 id="presence-of-highly-correlated-features">Presence of Highly Correlated Features<a class="headerlink" href="#presence-of-highly-correlated-features" title="Permanent link">&para;</a></h4>
<ul>
<li>Ridge: it generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.</li>
<li>Lasso: it arbitrarily selects any one feature among the highly correlated ones and reduced the coefficients of the rest to zero. Also, the chosen variable changes randomly with changes in model parameters. This generally doesn&rsquo;t work that well as compared to ridge regression.</li>
</ul>
<p>This disadvantage of lasso can be observed in the example we discussed above.<br />
Since we used a polynomial regression, the variables were highly correlated. (Not sure why? Check the output of <code class="codehilite">data.corr()</code>).<br />
Thus, we saw that even small values of alpha were giving significant sparsity (i.e. high numbers of coefficients as zero).</p>
<p>Along with Ridge and Lasso, Elastic Net is another useful technique which combines both L1 and L2 regularization. It can be used to balance out the pros and cons of ridge and lasso regression. I encourage you to explore it further.</p>
<h4 id="elasticnet-regression">ElasticNet Regression<a class="headerlink" href="#elasticnet-regression" title="Permanent link">&para;</a></h4>
<p>ElasticNet is hybrid of Lasso and Ridge Regression techniques.<br />
It is trained with L1 and L2 prior as regularizers.<br />
Elastic-net is useful when there are multiple features which are correlated.<br />
Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.</p>
<p><strong>Objective = RSS + alpha1 * (sum of square of coefficients) + alpha2 * (sum of absolute value of coefficients)</strong></p>
<p>Elastic-Net to inherit some of Ridge&rsquo;s stability under rotation.<br />
It encourages group effect in case of highly correlated variables.<br />
There are no limitations on the number of selected variables.<br />
It can suffer from double shrinkage.</p>
<h3 id="data-reduction-techniques">Data reduction techniques<a class="headerlink" href="#data-reduction-techniques" title="Permanent link">&para;</a></h3>
<p>Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.<br />
Generally, this is called a data reduction technique.<br />
A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Importing your necessary module</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Feature extraction</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Summarizing components</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Explained Variance: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Explained Variance: [0.88854663 0.06159078 0.02579012]
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02
   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]
 [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02
   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]
 [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01
   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]
</pre></div>
</td></tr></table>

<p>The transformed dataset (3 principal components) bare little resemblance to the source data.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../Exploratory Data Analysis/" title="Exploratory Data Analysis (EDA)" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Exploratory Data Analysis (EDA)
              </span>
            </div>
          </a>
        
        
          <a href="../Geospatial_Data_in_Python/" title="Geospatial Data" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Geospatial Data
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            © Ugo Sparks
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
        <script>!function(e,a,t,n,o,c,i){e.GoogleAnalyticsObject=o,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),i=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",i.parentNode.insertBefore(c,i)}(window,document,"script",0,"ga"),ga("create","UA-93008985-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview");var links=document.getElementsByTagName("a");if(Array.prototype.map.call(links,function(e){e.host!=document.location.host&&e.addEventListener("click",function(){var a=e.getAttribute("data-md-action")||"follow";ga("send","event","outbound",a,e.href)})}),document.forms.search){var query=document.forms.search.query;query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}</script>
      
    
  </body>
</html>