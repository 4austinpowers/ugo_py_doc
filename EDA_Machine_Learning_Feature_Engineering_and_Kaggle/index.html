<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Ugo Sparks">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>EDA, Machine Learning, Feature Engineering, and Kaggle - ugo_py_doc</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "EDA, Machine Learning, Feature Engineering, and Kaggle";
    var mkdocs_page_input_path = "EDA_Machine_Learning_Feature_Engineering_and_Kaggle.md";
    var mkdocs_page_url = "/EDA_Machine_Learning_Feature_Engineering_and_Kaggle/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-93008985-2', 'auto');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ugo_py_doc</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Basics & More</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Py_CS/">Python Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python_Preliminaries/">Python Preliminaries</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python_Nice_to_Have/">Python Nice to Have</a>
                </li>
                <li class="">
                    
    <a class="" href="../Freeze_the_Code/">Freeze the Code</a>
                </li>
                <li class="">
                    
    <a class="" href="../Decorators/">Decorators</a>
                </li>
                <li class="">
                    
    <a class="" href="../Write_Better_Python/">Write Better Python with PEP</a>
                </li>
                <li class="">
                    
    <a class="" href="../Regex/">Regular Expressions (REGEX)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Databases/">Databases</a>
                </li>
                <li class="">
                    
    <a class="" href="../Datetime/">Datetime</a>
                </li>
                <li class="">
                    
    <a class="" href="../Execute_Highlighted_Python_Code_in_gedit/">Execute Highlighted Python Code in gedit</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">SciPy Stack</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Scipy_CS/">Scipy Stack Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../JN_CS/">Jupyter Notebook Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Scientific Python (the SciPy Stack)/">Scientific Python (the SciPy Stack)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Importing Data into Python/">Importing Data into Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python for Data Science/">Python for Data Science</a>
                </li>
                <li class="">
                    
    <a class="" href="../Tidy_Data_in_Python/">Tidy Data in Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Lists/">Lists</a>
                </li>
                <li class="">
                    
    <a class="" href="../IPython Notebook/">IPython Notebook, Collection</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python Numpy Arrays/">Python Numpy Arrays</a>
                </li>
                <li class="">
                    
    <a class="" href="../Vectors and Arrays (Linear Algebra)/">Vectors and Arrays (Linear Algebra)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Matplotlib, Python Plotting/">Matplotlib, Python Plotting</a>
                </li>
                <li class="">
                    
    <a class="" href="../Viewing+3D+Volumetric+Data+With+Matplotlib/">Viewing 3D Volumetric Data With Matplotlib</a>
                </li>
                <li class="">
                    
    <a class="" href="../Seaborn, Python Statistical Data Visualization Library/">Seaborn, Python's Statistical Data Visualization Library</a>
                </li>
                <li class="">
                    
    <a class="" href="../Pandas+DataFrames/">Pandas DataFrames</a>
                </li>
                <li class="">
                    
    <a class="" href="../Write Idiomatic Pandas Code/">Write Idiomatic Pandas Code</a>
                </li>
                <li class="">
                    
    <a class="" href="../Exploratory Data Analysis/">Exploratory Data Analysis</a>
                </li>
                <li class="">
                    
    <a class="" href="../Intro to data.world in Python/">Intro to data.world in Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python+And+Excel/">Python and Excel</a>
                </li>
                <li class="">
                    
    <a class="" href="../Overview_of_scikit-learn/">Overview of scikit-learn</a>
                </li>
                <li class="">
                    
    <a class="" href="../k-NN_Linear_regression_Logit_Scaling_Centering_Noise/">k-NN, Linear regression, Logit, Scaling, Centering, Noise</a>
                </li>
                <li class="">
                    
    <a class="" href="../Time_Series_Analysis/">Time Series Analysis</a>
                </li>
                <li class="">
                    
    <a class="" href="../Sentiment_Analysis_with_Twitter/">Sentiment Analysis with Twitter</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">EDA, Machine Learning, Feature Engineering, and Kaggle</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#how-to-start-with-supervised-learning-take-1">How to Start with Supervised Learning (Take 1)</a></li>
    

    <li class="toctree-l3"><a href="#import-the-data-and-explore-it">Import the Data and Explore it</a></li>
    

    <li class="toctree-l3"><a href="#visual-exploratory-data-analysis-eda-and-a-first-model">Visual Exploratory Data Analysis (EDA) and a First Model</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#submit-to-kaggle-1st">Submit to Kaggle (1st)</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#eda-on-feature-variables">EDA on Feature Variables</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#submit-to-kaggle-2nd">Submit to Kaggle (2nd)</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#explore-the-data-more">Explore the Data More!</a></li>
    

    <li class="toctree-l3"><a href="#eda-with-numeric-variables">EDA with Numeric Variables</a></li>
    

    <li class="toctree-l3"><a href="#a-first-machine-learning-model">A First Machine Learning Model</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#how-to-start-with-supervised-learning-take-2">How to Start with Supervised Learning (Take 2)</a></li>
        
            <li><a class="toctree-l4" href="#build-a-decision-tree-classifier">Build a Decision Tree Classifier</a></li>
        
            <li><a class="toctree-l4" href="#a-decision-tree-classifier-in-more-details">A Decision Tree Classifier in More Details</a></li>
        
            <li><a class="toctree-l4" href="#why-choose-max_depth3">Why Choose max_depth=3?</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#feature-engineering">Feature Engineering</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#how-to-start-with-feature-engineering">How to Start with Feature Engineering</a></li>
        
            <li><a class="toctree-l4" href="#why-feature-engineer-at-all">Why Feature Engineer At All?</a></li>
        
            <li><a class="toctree-l4" href="#building-models-with-a-new-dataset">Building models with a New Dataset!</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Courses</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Apprenez a programmer en Python/">Apprenez à programmer en Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Codecademy Python/">Codecademy Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Learn Python the Hard Way/">Learn Python the Hard Way</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python Code Snippets/">Python Code Snippets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Introduction to Python/">Introduction to Python</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Manuals</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Automate the Boring Stuff with Python/">Automate the Boring Stuff with Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Real_Python/">Real Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Managing Your Biological Data with Python/">Managing Your Biological Data with Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python for Education/">Python for Education</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ugo_py_doc</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>SciPy Stack &raquo;</li>
        
      
    
    <li>EDA, Machine Learning, Feature Engineering, and Kaggle</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <div class="toc"><span class="toctitle">CONTENT</span><ul>
<li><a href="#exploratory-data-analysis-eda-prior-to-machine-learning">Exploratory Data Analysis (EDA) prior to Machine Learning</a><ul>
<li><a href="#how-to-start-with-supervised-learning-take-1">How to Start with Supervised Learning (Take 1)</a></li>
<li><a href="#import-the-data-and-explore-it">Import the Data and Explore it</a></li>
<li><a href="#visual-exploratory-data-analysis-eda-and-a-first-model">Visual Exploratory Data Analysis (EDA) and a First Model</a><ul>
<li><a href="#submit-to-kaggle-1st">Submit to Kaggle (1st)</a></li>
</ul>
</li>
<li><a href="#eda-on-feature-variables">EDA on Feature Variables</a><ul>
<li><a href="#submit-to-kaggle-2nd">Submit to Kaggle (2nd)</a></li>
</ul>
</li>
<li><a href="#explore-the-data-more">Explore the Data More!</a></li>
<li><a href="#eda-with-numeric-variables">EDA with Numeric Variables</a></li>
</ul>
</li>
<li><a href="#a-first-machine-learning-model">A First Machine Learning Model</a><ul>
<li><a href="#how-to-start-with-supervised-learning-take-2">How to Start with Supervised Learning (Take 2)</a></li>
<li><a href="#build-a-decision-tree-classifier">Build a Decision Tree Classifier</a><ul>
<li><a href="#submit-to-kaggle-3rd">Submit to Kaggle (3rd)</a></li>
</ul>
</li>
<li><a href="#a-decision-tree-classifier-in-more-details">A Decision Tree Classifier in More Details</a></li>
<li><a href="#why-choose-max_depth3">Why Choose max_depth=3?</a></li>
</ul>
</li>
<li><a href="#feature-engineering">Feature Engineering</a><ul>
<li><a href="#how-to-start-with-feature-engineering">How to Start with Feature Engineering</a></li>
<li><a href="#why-feature-engineer-at-all">Why Feature Engineer At All?</a><ul>
<li><a href="#titanics-passenger-titles">Titanic&rsquo;s Passenger Titles</a></li>
<li><a href="#passengers-cabins">Passenger&rsquo;s Cabins</a></li>
<li><a href="#handling-missing-values">Handling Missing Values</a></li>
<li><a href="#binning-numerical-data">Binning Numerical Data</a></li>
<li><a href="#number-of-members-in-family-onboard">Number of Members in Family Onboard</a></li>
<li><a href="#transforming-all-variables-into-numerical-variables">Transforming all Variables into Numerical Variables</a></li>
</ul>
</li>
<li><a href="#building-models-with-a-new-dataset">Building models with a New Dataset!</a><ul>
<li><a href="#submit-to-kaggle-4th">Submit to Kaggle (4th)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<hr />
<p><strong>Foreword</strong></p>
<p>Code snippets and excerpts from the tutorial. Python 3. From DataCamp.</p>
<hr />
<h3 id="exploratory-data-analysis-eda-prior-to-machine-learning">Exploratory Data Analysis (EDA) prior to Machine Learning<a class="headerlink" href="#exploratory-data-analysis-eda-prior-to-machine-learning" title="Permanent link">&para;</a></h3>
<p>Supervised learning models with the help of exploratory data analysis (EDA) on the Titanic data.</p>
<h4 id="how-to-start-with-supervised-learning-take-1">How to Start with Supervised Learning (Take 1)<a class="headerlink" href="#how-to-start-with-supervised-learning-take-1" title="Permanent link">&para;</a></h4>
<p>Approach supervised learning is the following:</p>
<ul>
<li>Perform an Exploratory Data Analysis (EDA) on a dataset;</li>
<li>Build a quick and dirty model, or a baseline model, which can serve as a comparison against later models that we will build;</li>
<li>Iterate this process. We will do more EDA and build another model;</li>
<li>Engineer features: take the features that we already have and combine them or extract more information from them to eventually come to the last point, which is</li>
<li>Get a model that performs better.</li>
</ul>
<h4 id="import-the-data-and-explore-it">Import the Data and Explore it<a class="headerlink" href="#import-the-data-and-explore-it" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Import modules
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.metrics import accuracy_score

# Figures inline and set visualization style
%matplotlib inline
sns.set()
</code></pre>

<pre><code class="python"># Import test and train datasets
df_train = pd.read_csv('data/train.csv')
df_test = pd.read_csv('data/test.csv')

# View first lines of training data
df_train.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>The target variable is the variable we are trying to predict;</li>
<li>Other variables are known as &ldquo;features&rdquo; (or &ldquo;predictor variables&rdquo;, the features that we are using to predict the target variable).</li>
</ul>
<p>Note that the <code>df_test</code> DataFrame doesn&rsquo;t have the <code>Survived</code> column because this is what we will try to predict!</p>
<pre><code class="python"># View first lines of test data
df_test.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">df_train.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
</code></pre>
<pre><code class="python">df_train.describe()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>446.000000</td>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>257.353842</td>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>223.500000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>446.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>668.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>891.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id="visual-exploratory-data-analysis-eda-and-a-first-model">Visual Exploratory Data Analysis (EDA) and a First Model<a class="headerlink" href="#visual-exploratory-data-analysis-eda-and-a-first-model" title="Permanent link">&para;</a></h4>
<p>With seaborn.</p>
<pre><code class="python">sns.countplot(x='Survived', data=df_train)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc65fa0e668&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_9_1.png" /></p>
<p><strong>Take-away</strong>: in the training set, less people survived than didn&rsquo;t. Let&rsquo;s then build a first model that predicts that nobody survived.</p>
<p>This is a bad model as we know that people survived. But it gives us a <strong>baseline</strong>: any model that we build later needs to do better than this one.</p>
<ul>
<li>Create a column <code>Survived</code> for <code>df_test</code> that encodes &lsquo;did not survive&rsquo; for all rows;</li>
<li>Save <code>PassengerId</code> and <code>Survived</code> columns of <code>df_test</code> to a .csv and submit to Kaggle.</li>
</ul>
<pre><code class="python">df_test['Survived'] = 0
df_test[['PassengerId', 'Survived']].to_csv('results/no_survivors.csv', index=False)
</code></pre>

<h5 id="submit-to-kaggle-1st">Submit to Kaggle (1st)<a class="headerlink" href="#submit-to-kaggle-1st" title="Permanent link">&para;</a></h5>
<ul>
<li>Go to <a href="https://www.kaggle.com">Kaggle</a>, log in, and search for <em>Titanic: Machine Learning from Disaster</em>. </li>
<li>Join the competition and submit the .csv file.</li>
<li>Add a description and submit.</li>
<li>Kaggle returns a ranking.</li>
<li>At the time of the first submission: score 0.63679, rank 9387.</li>
</ul>
<p><img alt="" src="../results/no_survivors.png" /></p>
<h4 id="eda-on-feature-variables">EDA on Feature Variables<a class="headerlink" href="#eda-on-feature-variables" title="Permanent link">&para;</a></h4>
<p>Do some more Exploratory Data Analysis and build another model!</p>
<pre><code class="python">sns.countplot(x='Sex', data=df_train);
</code></pre>

<p><img alt="" src="../img/EDA_machine_learning/output_14_0.png" /></p>
<pre><code class="python"># kind is the facets
sns.factorplot(x='Survived', col='Sex', kind='count', data=df_train)
</code></pre>

<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x7fc65fa35a20&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_15_1.png" /></p>
<p><strong>Take-away</strong>: Women were more likely to survive than men.</p>
<p>With this take-away, we can use pandas to figure out how many women and how many men survived:</p>
<pre><code class="python">df_train.head(1)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.25</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Chain a group by Sex, sum Survived
df_train.groupby(['Sex']).Survived.sum()
</code></pre>

<pre><code>Sex
female    233
male      109
Name: Survived, dtype: int64
</code></pre>
<pre><code class="python"># Chain calculations
print(df_train[df_train.Sex == 'female'].Survived.sum() /
      df_train[df_train.Sex == 'female'].Survived.count())

print(df_train[df_train.Sex == 'male'].Survived.sum() /
      df_train[df_train.Sex == 'male'].Survived.count())
</code></pre>

<pre><code>0.742038216561
0.188908145581
</code></pre>
<p>74% of women survived, while 19% of men survived.</p>
<p>Build a second model and predict that all women survived and all men didn&rsquo;t.</p>
<ul>
<li>Create a column <code>Survived</code> for <code>df_test</code> that encodes the above prediction.</li>
<li>Save <code>PassengerId</code> and <code>Survived</code> columns of <code>df_test</code> to a .csv and submit to Kaggle.</li>
</ul>
<pre><code class="python">df_test['Survived'] = df_test.Sex == 'female'
df_test['Survived'] = df_test.Survived.apply(lambda x: int(x))
df_test.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">df_test[['PassengerId', 'Survived']].to_csv('results/women_survived.csv', index=False)
</code></pre>

<h5 id="submit-to-kaggle-2nd">Submit to Kaggle (2nd)<a class="headerlink" href="#submit-to-kaggle-2nd" title="Permanent link">&para;</a></h5>
<ul>
<li>Go to <a href="https://www.kaggle.com">Kaggle</a>, log in, and search for <em>Titanic: Machine Learning from Disaster</em>. </li>
<li>Join the competition and submit the .csv file.</li>
<li>Add a description and submit.</li>
<li>Kaggle returns a ranking.</li>
<li>At the time of the first submission: score 0.76555 (from 0.62679), rank 7274 (a jump of 2122 places).</li>
</ul>
<p><img alt="" src="../results/women_survived.png" /></p>
<h4 id="explore-the-data-more">Explore the Data More!<a class="headerlink" href="#explore-the-data-more" title="Permanent link">&para;</a></h4>
<pre><code class="python"># kind is the facets
sns.factorplot(x='Survived', col='Pclass', kind='count', data=df_train)
</code></pre>

<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x7fc65f8dcf98&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_25_1.png" /></p>
<p><strong>Take-away</strong>: Passengers that travelled in first class were more likely to survive. On the other hand, passengers travelling in third class were more unlikely to survive. </p>
<pre><code class="python"># kind is the facets
sns.factorplot(x='Survived', col='Embarked', kind='count', data=df_train)
</code></pre>

<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x7fc65f937c50&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_27_1.png" /></p>
<p><strong>Take-away</strong>: Passengers that embarked in Southampton were less likely to survive. </p>
<h4 id="eda-with-numeric-variables">EDA with Numeric Variables<a class="headerlink" href="#eda-with-numeric-variables" title="Permanent link">&para;</a></h4>
<pre><code class="python">sns.distplot(df_train.Fare, kde=False)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc6633fb9e8&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_30_1.png" /></p>
<p><strong>Take-away</strong>: Most passengers paid less than 100 for travelling with the Titanic.</p>
<pre><code class="python"># Group by Survived, trace histograms of Fare with alpha color 0.6
df_train.groupby('Survived').Fare.hist(alpha=0.6)
</code></pre>

<pre><code>Survived
0    Axes(0.125,0.125;0.775x0.775)
1    Axes(0.125,0.125;0.775x0.775)
Name: Fare, dtype: object
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_32_1.png" /></p>
<p><strong>Take-away</strong>: It looks as though those that paid more had a higher chance of surviving.</p>
<pre><code class="python"># Remove NaN
df_train_drop = df_train.dropna()

sns.distplot(df_train_drop.Age, kde=False)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc65f64be80&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_34_1.png" /></p>
<pre><code class="python"># Alternative to bars or scatter
sns.stripplot(x='Survived', 
              y='Fare', 
              data=df_train, 
              alpha=0.3, jitter=True)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc65f5cca58&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_35_1.png" /></p>
<pre><code class="python"># Alternative to bars or scatter
sns.swarmplot(x='Survived', 
              y='Fare', 
              data=df_train)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc65f2e4ef0&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_36_1.png" /></p>
<p><strong>Take-away</strong>: Fare definitely seems to be correlated with survival aboard the Titanic.</p>
<pre><code class="python"># Group by Survived, describe Fare (descriptive statistics)
df_train.groupby('Survived').Fare.describe()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Survived</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>549.0</td>
      <td>22.117887</td>
      <td>31.388207</td>
      <td>0.0</td>
      <td>7.8542</td>
      <td>10.5</td>
      <td>26.0</td>
      <td>263.0000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>342.0</td>
      <td>48.395408</td>
      <td>66.596998</td>
      <td>0.0</td>
      <td>12.4750</td>
      <td>26.0</td>
      <td>57.0</td>
      <td>512.3292</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">sns.lmplot(x='Age', 
           y='Fare', 
           hue='Survived', 
           data=df_train, 
           fit_reg=False, scatter_kws={'alpha':0.5})
</code></pre>

<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x7fc65f5ccba8&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_39_1.png" /></p>
<pre><code class="python">sns.lmplot(x='Age', 
           y='Fare', 
           hue='Survived', 
           data=df_train, 
           fit_reg=True, scatter_kws={'alpha':0.5})
</code></pre>

<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x7fc65f22d710&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_40_1.png" /></p>
<p><strong>Take-away</strong>: It looks like those who survived either paid quite a bit for their ticket or they were young.</p>
<pre><code class="python">sns.pairplot(df_train_drop, hue='Survived')
</code></pre>

<pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7fc65f8826d8&gt;
</code></pre>
<p><img alt="" src="../img/EDA_machine_learning/output_42_1.png" /></p>
<h3 id="a-first-machine-learning-model">A First Machine Learning Model<a class="headerlink" href="#a-first-machine-learning-model" title="Permanent link">&para;</a></h3>
<p>A decision tree classifier, with the Python scikit-learn.</p>
<h4 id="how-to-start-with-supervised-learning-take-2">How to Start with Supervised Learning (Take 2)<a class="headerlink" href="#how-to-start-with-supervised-learning-take-2" title="Permanent link">&para;</a></h4>
<p>Now that we have done our homeworks with EDA&hellip;</p>
<pre><code class="python"># Import modules
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import numpy as np
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Figures inline and set visualization style
%matplotlib inline
sns.set()
</code></pre>

<pre><code class="python"># Import data
df_train = pd.read_csv('data/train.csv')
df_test = pd.read_csv('data/test.csv')
</code></pre>

<pre><code class="python">df_train.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
</code></pre>
<pre><code class="python">df_test.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 11 columns):
PassengerId    418 non-null int64
Pclass         418 non-null int64
Name           418 non-null object
Sex            418 non-null object
Age            332 non-null float64
SibSp          418 non-null int64
Parch          418 non-null int64
Ticket         418 non-null object
Fare           417 non-null float64
Cabin          91 non-null object
Embarked       418 non-null object
dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB
</code></pre>
<pre><code class="python"># Store target variable of training data in a safe place
survived_train = df_train.Survived

# Concatenate (along the index or axis=1) training and test sets
# to preprocess the data a little bit
# and make sure that any operations that
# we perform on the training set are also
# being done on the test data set
data = pd.concat([df_train.drop(['Survived'], axis=1), df_test])
</code></pre>

<pre><code class="python"># The combined datasets (891+418 entries)
data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 11 columns):
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Name           1309 non-null object
Sex            1309 non-null object
Age            1046 non-null float64
SibSp          1309 non-null int64
Parch          1309 non-null int64
Ticket         1309 non-null object
Fare           1308 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
dtypes: float64(2), int64(4), object(5)
memory usage: 122.7+ KB
</code></pre>
<p>Missing values for the <code>Age</code> and <code>Fare</code> columns! Also notice that <code>Cabin</code> and <code>Embarked</code> are also missing values and we will need to deal with that also at some point. However, now we will focus on fixing the numerical variables <code>Age</code> and <code>Fare</code>, using the median of the of these variables where we know them. It&rsquo;s perfect for dealing with outliers. In other words, the median is useful to use when the distribution of data is skewed. Other ways to impute the missing values would be to use the mean or the mode.</p>
<pre><code class="python"># Impute missing numerical variables where NaN
data['Age'] = data.Age.fillna(data.Age.median())
data['Fare'] = data.Fare.fillna(data.Fare.median())

# Check out info of data
data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 11 columns):
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Name           1309 non-null object
Sex            1309 non-null object
Age            1309 non-null float64
SibSp          1309 non-null int64
Parch          1309 non-null int64
Ticket         1309 non-null object
Fare           1309 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
dtypes: float64(2), int64(4), object(5)
memory usage: 122.7+ KB
</code></pre>
<p>Encode the data with numbers with <code>.get_dummies()</code>. </p>
<p>It creates a new column for female, called <code>Sex_female</code>, and then a new column for <code>Sex_male</code>, which encodes whether that row was male or female (1 if that row is a male - and a 0 if that row is female). Because of <code>drop_first</code> argument, we dropped <code>Sex_female</code> because, essentially, these new columns, <code>Sex_female</code> and <code>Sex_male</code>, encode the same information.</p>
<pre><code class="python">data = pd.get_dummies(data, columns=['Sex'], drop_first=True)
data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Sex_male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Select columns and view head
data = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]
data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sex_male</th>
      <th>Fare</th>
      <th>Age</th>
      <th>Pclass</th>
      <th>SibSp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>7.2500</td>
      <td>22.0</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>71.2833</td>
      <td>38.0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>7.9250</td>
      <td>26.0</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 5 columns):
Sex_male    1309 non-null uint8
Fare        1309 non-null float64
Age         1309 non-null float64
Pclass      1309 non-null int64
SibSp       1309 non-null int64
dtypes: float64(2), int64(2), uint8(1)
memory usage: 52.4 KB
</code></pre>
<p>All the entries are non-null now.</p>
<h4 id="build-a-decision-tree-classifier">Build a Decision Tree Classifier<a class="headerlink" href="#build-a-decision-tree-classifier" title="Permanent link">&para;</a></h4>
<p>&ldquo;Was <code>Sex_male</code>&rdquo; less than 0.5? In other words, was the data point a female. If the answer to this question is <code>True</code>, we can go down to the left and we get <code>Survived</code>. If <code>False</code>, we go down the right and we get <code>Dead</code>.</p>
<p><img alt="" src="../img/decision_tree_titanic_1.png" /></p>
<p>That the first branch is on <code>Male</code> or not and that <code>Male</code> results in a prediction of <code>Dead</code>. The gini coefficient is used to make these decisions.</p>
<p>Before fitting a model to the data, split it back into training and test sets:</p>
<pre><code class="python">data_train = data.iloc[:891]
data_test = data.iloc[891:]
</code></pre>

<p>scikit-learn requires the data as arrays, not DataFrames. Transform them.</p>
<pre><code class="python">X = data_train.values
test = data_test.values

# and from above: survived_train = df_train.Survived
y = survived_train.values
</code></pre>

<pre><code class="python">X
</code></pre>

<pre><code>array([[  1.    ,   7.25  ,  22.    ,   3.    ,   1.    ],
       [  0.    ,  71.2833,  38.    ,   1.    ,   1.    ],
       [  0.    ,   7.925 ,  26.    ,   3.    ,   0.    ],
       ..., 
       [  0.    ,  23.45  ,  28.    ,   3.    ,   1.    ],
       [  1.    ,  30.    ,  26.    ,   1.    ,   0.    ],
       [  1.    ,   7.75  ,  32.    ,   3.    ,   0.    ]])
</code></pre>
<p>Build a decision tree classifier! First create such a model with <code>max_depth=3</code> and then fit it the data. Name the model <code>clf</code>, which is short for &ldquo;Classifier&rdquo;.</p>
<pre><code class="python"># Instantiate model and fit to data
# The max depth is set at 3
clf = tree.DecisionTreeClassifier(max_depth=3)

# X is the indenpendent variables, y is the dependent variable
clf.fit(X, y)
</code></pre>

<pre><code>DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
</code></pre>
<p>Make predictions on the test set.</p>
<pre><code class="python"># Make predictions and store in 'Survived' column of df_test
Y_pred = clf.predict(test)
df_test['Survived'] = Y_pred

# Save it
df_test[['PassengerId', 'Survived']].to_csv('results/1st_dec_tree.csv',
                                            index=False)
</code></pre>

<h5 id="submit-to-kaggle-3rd">Submit to Kaggle (3rd)<a class="headerlink" href="#submit-to-kaggle-3rd" title="Permanent link">&para;</a></h5>
<ul>
<li>Go to <a href="https://www.kaggle.com">Kaggle</a>, log in, and search for <em>Titanic: Machine Learning from Disaster</em>. </li>
<li>Join the competition and submit the .csv file.</li>
<li>Add a description and submit.</li>
<li>Kaggle returns a ranking.</li>
<li>At the time of the first submission: score 0.77990 (from 0.76555), rank 4828 (a jump of 2434 places).</li>
</ul>
<p><img alt="" src="../results/1st_dec_tree.png" /></p>
<pre><code class="python"># Compute accuracy on the training set
train_accuracy = clf.score(X, y)
train_accuracy
</code></pre>

<pre><code>0.8271604938271605
</code></pre>
<h4 id="a-decision-tree-classifier-in-more-details">A Decision Tree Classifier in More Details<a class="headerlink" href="#a-decision-tree-classifier-in-more-details" title="Permanent link">&para;</a></h4>
<p>The Decision Tree Classifier we just built had a max_depth=3 and it looks like this:</p>
<p><img alt="" src="../img/decision_tree_titanic_3.png" /></p>
<p>The maximal distance between the first decision and the last is 3, so that&rsquo;s <code>max_depth=3</code>.</p>
<p>Generate images with <a href="http://scikit-learn.org/stable/modules/tree.html">graphviz</a>.</p>
<pre><code class="python">import graphviz

tree_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(tree_data)
# Save the pdf
graph.render(&quot;img/tree_data&quot;)
</code></pre>

<pre><code>'img/tree_data.pdf'
</code></pre>
<p>We get a tree_data test file (the code for generating the image) and a pdf file. We can generate an image.</p>
<pre><code class="python">feature_names = list(data_train)
feature_names
</code></pre>

<pre><code>['Sex_male', 'Fare', 'Age', 'Pclass', 'SibSp']
</code></pre>
<pre><code class="python">#data_train
#data_test
tree_data = tree.export_graphviz(clf, out_file=None, 
                                feature_names=feature_names,
                                class_names=None,
                                filled=True, rounded=True,
                                special_characters=True)  
graph = graphviz.Source(tree_data)  
graph 
</code></pre>

<p><img alt="svg" src="../output_72_0.svg" /></p>
<p>In building this model, what we are essentially doing is creating a <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html">decision boundary</a> in the space of feature variables.</p>
<p><img alt="" src="../img/dec_bound.png" /></p>
<h4 id="why-choose-max_depth3">Why Choose <code>max_depth=3</code>?<a class="headerlink" href="#why-choose-max_depth3" title="Permanent link">&para;</a></h4>
<p>The depth of the tree is known as a hyperparameter, which means a parameter we need to decide before we fit the model to the data. If we choose a larger <code>max_depth</code>, we will get a more complex decision boundary;  the bias-variance trade-off.</p>
<ul>
<li>If the decision boundary is too complex, we can overfit to the data, which means that the model will be describing noise as well as signal.</li>
<li>If the <code>max_depth</code> is too small, we might be underfitting the data, meaning that the model doesn&rsquo;t contain enough of the signal.</li>
</ul>
<p>One way is to hold out a test set from the training data. We can then fit the model to the training data, make predictions on the test set and see how well the prediction does on the test set.</p>
<p>Split the original training data into training and test sets:</p>
<pre><code class="python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)
</code></pre>

<p>Iterate over values of <code>max_depth</code> ranging from 1 to 9 and plot the accuracy of the models on training and test sets:</p>
<pre><code class="python"># Setup arrays to store train and test accuracies
dep = np.arange(1, 9)
train_accuracy = np.empty(len(dep))
test_accuracy = np.empty(len(dep))

# Loop over different values of k
for i, k in enumerate(dep):
    # Setup a k-NN Classifier with k neighbors: knn
    clf = tree.DecisionTreeClassifier(max_depth=k)

    # Fit the classifier to the training data
    clf.fit(X_train, y_train)

    # Compute accuracy on the training set
    train_accuracy[i] = clf.score(X_train, y_train)

    # Compute accuracy on the testing set
    test_accuracy[i] = clf.score(X_test, y_test)

# Generate plot
plt.title('clf: Varying depth of tree')
plt.plot(dep, test_accuracy, label = 'Testing Accuracy')
plt.plot(dep, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Depth of tree')
plt.ylabel('Accuracy')
plt.show()
</code></pre>

<p><img alt="" src="../img/EDA_machine_learning/output_76_0.png" /></p>
<p>At <code>max_depth-3</code>, we get the same results as with the model before (around 82%).</p>
<p>As we increase the max_depth, we are going to fit better and better to the training data because we will make decisions that describe the training data. The accuracy for the training data will go up and up, but we see that this doesn&rsquo;t happen for the test data: we are overfitting.</p>
<p>So that&rsquo;s why we chose <code>max_depth=3</code>. </p>
<h3 id="feature-engineering">Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permanent link">&para;</a></h3>
<p>https://www.datacamp.com/community/tutorials/feature-engineering-kaggle</p>
<p>A process where we use domain knowledge of the data to create additional relevant features (create new columns, transform variables and more) that increase the predictive power of the learning algorithm and make the machine learning models perform even better.</p>
<h4 id="how-to-start-with-feature-engineering">How to Start with Feature Engineering<a class="headerlink" href="#how-to-start-with-feature-engineering" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import numpy as np
from sklearn import tree
from sklearn.model_selection import GridSearchCV

# Figures inline and set visualization style
%matplotlib inline
sns.set()
</code></pre>

<pre><code class="python"># Import data
df_train = pd.read_csv('data/train.csv')
df_test = pd.read_csv('data/test.csv')

# Store target variable of training data in a safe place
survived_train = df_train.Survived

# Concatenate training and test sets
data = pd.concat([df_train.drop(['Survived'], axis=1), df_test])

# View head
data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 11 columns):
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Name           1309 non-null object
Sex            1309 non-null object
Age            1046 non-null float64
SibSp          1309 non-null int64
Parch          1309 non-null int64
Ticket         1309 non-null object
Fare           1308 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
dtypes: float64(2), int64(4), object(5)
memory usage: 122.7+ KB
</code></pre>
<h4 id="why-feature-engineer-at-all">Why Feature Engineer At All?<a class="headerlink" href="#why-feature-engineer-at-all" title="Permanent link">&para;</a></h4>
<h5 id="titanics-passenger-titles">Titanic&rsquo;s Passenger Titles<a class="headerlink" href="#titanics-passenger-titles" title="Permanent link">&para;</a></h5>
<pre><code class="python"># View head of 'Name' column
data.Name.tail()
</code></pre>

<pre><code>413              Spector, Mr. Woolf
414    Oliva y Ocana, Dona. Fermina
415    Saether, Mr. Simon Sivertsen
416             Ware, Mr. Frederick
417        Peter, Master. Michael J
Name: Name, dtype: object
</code></pre>
<p>These titles of course give us information on social status, profession, etc., which in the end could tell us something more about survival. use regular expressions to extract the title and store it in a new column &lsquo;Title&rsquo;:</p>
<pre><code class="python"># Extract Title from Name, store in column and plot barplot
# One upper character, one lower character, one dot
data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))
</code></pre>

<pre><code class="python"># New column Title is a new feature of the dataset 
data.Title.head(3)
</code></pre>

<pre><code>0      Mr
1     Mrs
2    Miss
Name: Title, dtype: object
</code></pre>
<pre><code class="python">sns.countplot(x='Title', data=data);
plt.xticks(rotation=45);
</code></pre>

<p><img alt="" src="../img/EDA_machine_learning/output_85_0.png" /></p>
<pre><code class="python"># Substitute some title with their English form
data['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})
# Gather exceptions
data['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')
</code></pre>

<pre><code class="python">data.Title.head(3)
</code></pre>

<pre><code>0      Mr
1     Mrs
2    Miss
Name: Title, dtype: object
</code></pre>
<pre><code class="python">sns.countplot(x='Title', data=data);
plt.xticks(rotation=45);
</code></pre>

<p><img alt="" src="../img/EDA_machine_learning/output_88_0.png" /></p>
<pre><code class="python"># View tail of data (for change)
data.tail(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>415</th>
      <td>1307</td>
      <td>3</td>
      <td>Saether, Mr. Simon Sivertsen</td>
      <td>male</td>
      <td>38.5</td>
      <td>0</td>
      <td>0</td>
      <td>SOTON/O.Q. 3101262</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
      <td>Mr</td>
    </tr>
    <tr>
      <th>416</th>
      <td>1308</td>
      <td>3</td>
      <td>Ware, Mr. Frederick</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>359309</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
      <td>Mr</td>
    </tr>
    <tr>
      <th>417</th>
      <td>1309</td>
      <td>3</td>
      <td>Peter, Master. Michael J</td>
      <td>male</td>
      <td>NaN</td>
      <td>1</td>
      <td>1</td>
      <td>2668</td>
      <td>22.3583</td>
      <td>NaN</td>
      <td>C</td>
      <td>Master</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="passengers-cabins">Passenger&rsquo;s Cabins<a class="headerlink" href="#passengers-cabins" title="Permanent link">&para;</a></h5>
<p>There are several NaNs or missing values in the <code>Cabin</code> column. Those NaNs didn&rsquo;t have a cabin, which could tell us something about survival.</p>
<pre><code class="python"># View head of data
data[['Name', 'PassengerId', 'Ticket', 'Cabin']].head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>PassengerId</th>
      <th>Ticket</th>
      <th>Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Braund, Mr. Owen Harris</td>
      <td>1</td>
      <td>A/5 21171</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>2</td>
      <td>PC 17599</td>
      <td>C85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Heikkinen, Miss. Laina</td>
      <td>3</td>
      <td>STON/O2. 3101282</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>4</td>
      <td>113803</td>
      <td>C123</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Allen, Mr. William Henry</td>
      <td>5</td>
      <td>373450</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Did they have a Cabin?
# Return True is the passenger has a cabin
data['Has_Cabin'] = ~data.Cabin.isnull()

# # View head of data
data[['Name', 'PassengerId', 'Ticket', 'Cabin', 'Has_Cabin']].head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>PassengerId</th>
      <th>Ticket</th>
      <th>Cabin</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Braund, Mr. Owen Harris</td>
      <td>1</td>
      <td>A/5 21171</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>2</td>
      <td>PC 17599</td>
      <td>C85</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Heikkinen, Miss. Laina</td>
      <td>3</td>
      <td>STON/O2. 3101282</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>4</td>
      <td>113803</td>
      <td>C123</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Allen, Mr. William Henry</td>
      <td>5</td>
      <td>373450</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<p>Drop these columns, except <code>Has_Cabin</code>, in the actual <code>data</code> DataFrame; make sure to use the <code>inplace</code> argument in the <code>.drop()</code> method and set it to <code>True</code>:</p>
<pre><code class="python"># Drop columns and view head
data.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1, inplace=True)
data.head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>Mrs</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Miss</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>Mrs</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<p>New features such as <code>Title</code> and <code>Has_Cabin</code>. </p>
<p>Features that don&rsquo;t add any more useful information for the machine learning model are now dropped from the DataFrame.</p>
<h5 id="handling-missing-values">Handling Missing Values<a class="headerlink" href="#handling-missing-values" title="Permanent link">&para;</a></h5>
<pre><code class="python">data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 9 columns):
Pclass       1309 non-null int64
Sex          1309 non-null object
Age          1046 non-null float64
SibSp        1309 non-null int64
Parch        1309 non-null int64
Fare         1308 non-null float64
Embarked     1307 non-null object
Title        1309 non-null object
Has_Cabin    1309 non-null bool
dtypes: bool(1), float64(2), int64(3), object(3)
memory usage: 133.3+ KB
</code></pre>
<p>Missing values in <code>Age</code>, <code>Fare</code>, and <code>Embarked</code>. Impute these missing values with the help of <code>.fillna()</code> and use the median to fill in the columns (or the mean, the mode, etc.).</p>
<pre><code class="python"># Impute missing values for Age, Fare, Embarked
data['Age'] = data.Age.fillna(data.Age.median())
data['Fare'] = data.Fare.fillna(data.Fare.median())
data['Embarked'] = data['Embarked'].fillna('S')
data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 9 columns):
Pclass       1309 non-null int64
Sex          1309 non-null object
Age          1309 non-null float64
SibSp        1309 non-null int64
Parch        1309 non-null int64
Fare         1309 non-null float64
Embarked     1309 non-null object
Title        1309 non-null object
Has_Cabin    1309 non-null bool
dtypes: bool(1), float64(2), int64(3), object(3)
memory usage: 133.3+ KB
</code></pre>
<pre><code class="python">data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>Mrs</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Miss</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="binning-numerical-data">Binning Numerical Data<a class="headerlink" href="#binning-numerical-data" title="Permanent link">&para;</a></h5>
<pre><code class="python"># Binning numerical columns
# q=4 means 4 quantiles 0, 1, 2, 3
# labels=False are numbers, not characters
data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
data['CatFare']= pd.qcut(data.Fare, q=4, labels=False)
data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Has_Cabin</th>
      <th>CatAge</th>
      <th>CatFare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>Mrs</td>
      <td>True</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Miss</td>
      <td>False</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Drop the 'Age' and 'Fare' columns
data = data.drop(['Age', 'Fare'], axis=1)
data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Has_Cabin</th>
      <th>CatAge</th>
      <th>CatFare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>1</td>
      <td>0</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>1</td>
      <td>0</td>
      <td>C</td>
      <td>Mrs</td>
      <td>True</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>0</td>
      <td>0</td>
      <td>S</td>
      <td>Miss</td>
      <td>False</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="number-of-members-in-family-onboard">Number of Members in Family Onboard<a class="headerlink" href="#number-of-members-in-family-onboard" title="Permanent link">&para;</a></h5>
<p>Create a new column, which is the number of members in families that were onboard of the Titanic.</p>
<pre><code class="python"># Create column of number of Family members onboard
data['Fam_Size'] = data.Parch + data.SibSp

# Drop columns
data = data.drop(['SibSp','Parch'], axis=1)
data.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Has_Cabin</th>
      <th>CatAge</th>
      <th>CatFare</th>
      <th>Fam_Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>S</td>
      <td>Mr</td>
      <td>False</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>C</td>
      <td>Mrs</td>
      <td>True</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>S</td>
      <td>Miss</td>
      <td>False</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<h5 id="transforming-all-variables-into-numerical-variables">Transforming all Variables into Numerical Variables<a class="headerlink" href="#transforming-all-variables-into-numerical-variables" title="Permanent link">&para;</a></h5>
<p>Transform all variables into numeric ones. We do this because machine learning models generally take numeric input.</p>
<pre><code class="python"># Transform into binary variables
# Has_Cabin is a boolean
# Sex becomes Sex_male=1 or 0
# Embarked becomes Embarked_Q=1 or 0, Embarked_...
# Title becomes Title_Miss=1 or 0, ...
# The former variables are dropped, only the later variables remain
data_dum = pd.get_dummies(data, drop_first=True)
data_dum.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Has_Cabin</th>
      <th>CatAge</th>
      <th>CatFare</th>
      <th>Fam_Size</th>
      <th>Sex_male</th>
      <th>Embarked_Q</th>
      <th>Embarked_S</th>
      <th>Title_Miss</th>
      <th>Title_Mr</th>
      <th>Title_Mrs</th>
      <th>Title_Special</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>False</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>True</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>False</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>First, split the data back into training and test sets. Then, transform them into arrays:</p>
<pre><code class="python"># Split into test.train
data_train = data_dum.iloc[:891]
data_test = data_dum.iloc[891:]

# Transform into arrays for scikit-learn
X = data_train.values
test = data_test.values
y = survived_train.values
</code></pre>

<h4 id="building-models-with-a-new-dataset">Building models with a New Dataset!<a class="headerlink" href="#building-models-with-a-new-dataset" title="Permanent link">&para;</a></h4>
<p>Build a decision tree on a brand new feature-engineered dataset. To choose the hyperparameter <code>max_depth</code>, we will use a variation on test train split called &ldquo;cross validation&rdquo;.</p>
<p><img alt="" src="../img/cv_raxrt7.png" /></p>
<p>Split the dataset into 5 groups or folds. Then we hold out the first fold as a test set, fit the model on the remaining four folds, predict on the test set and compute the metric of interest. Next, we hold out the second fold as the test set, fit on the remaining data, predict on the test set and compute the metric of interest. Then similarly with the third, fourth and fifth.</p>
<p>As a result, we get five values of accuracy, from which we can compute statistics of interest, such as the median and/or mean and 95% confidence intervals.</p>
<p>We do this for each value of each hyperparameter that we are tuning and choose the set of hyperparameters that performs the best. This is called grid search.</p>
<p>In the following, we will use cross validation and grid search to choose the best <code>max_depth</code> for the new feature-engineered dataset:</p>
<pre><code class="python"># Setup the hyperparameter grid
dep = np.arange(1,9)
param_grid = {'max_depth' : dep}

# Instantiate a decision tree classifier: clf
clf = tree.DecisionTreeClassifier()

# Instantiate the GridSearchCV object: clf_cv
clf_cv = GridSearchCV(clf, param_grid=param_grid, cv=5)

# Fit it to the data
clf_cv.fit(X, y)

# Print the tuned parameter and score
print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(clf_cv.best_params_))
print(&quot;Best score is {}&quot;.format(clf_cv.best_score_))
</code></pre>

<pre><code>Tuned Decision Tree Parameters: {'max_depth': 3}
Best score is 0.8294051627384961
</code></pre>
<p>Make predictions on the test set, create a new column <code>Survived</code> and store the predictions in it.</p>
<p>Save the <code>PassengerId</code> and <code>Survived</code> columns of <code>df_test</code> to a .csv and submit it to Kaggle.</p>
<pre><code class="python">Y_pred = clf_cv.predict(test)
df_test['Survived'] = Y_pred
df_test[['PassengerId', 'Survived']].to_csv('results/dec_tree_feat_eng.csv', index=False)
</code></pre>

<h5 id="submit-to-kaggle-4th">Submit to Kaggle (4th)<a class="headerlink" href="#submit-to-kaggle-4th" title="Permanent link">&para;</a></h5>
<ul>
<li>Go to <a href="https://www.kaggle.com">Kaggle</a>, log in, and search for <em>Titanic: Machine Learning from Disaster</em>. </li>
<li>Join the competition and submit the .csv file.</li>
<li>Add a description and submit.</li>
<li>Kaggle returns a ranking.</li>
<li>At the time of the first submission: score 0.78468 (from 0.77980), rank 4009 (a jump of 819 places).</li>
</ul>
<p><img alt="" src="../results/dec_tree_feat_enge.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Apprenez a programmer en Python/" class="btn btn-neutral float-right" title="Apprenez à programmer en Python">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Sentiment_Analysis_with_Twitter/" class="btn btn-neutral" title="Sentiment Analysis with Twitter"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>© Ugo Sparks</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Sentiment_Analysis_with_Twitter/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Apprenez a programmer en Python/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../mathjaxhelper.js"></script>

</body>
</html>
