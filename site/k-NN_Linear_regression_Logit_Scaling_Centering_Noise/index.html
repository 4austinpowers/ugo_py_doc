<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Ugo Sparks">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>k-NN, Linear regression, Logit, Scaling, Centering, Noise - ugo_py_doc</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "k-NN, Linear regression, Logit, Scaling, Centering, Noise";
    var mkdocs_page_input_path = "k-NN_Linear_regression_Logit_Scaling_Centering_Noise.md";
    var mkdocs_page_url = "/k-NN_Linear_regression_Logit_Scaling_Centering_Noise/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-93008985-2', 'auto');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ugo_py_doc</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Basics & More</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Py_CS/">Python Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python_Preliminaries/">Python Preliminaries</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python_Nice_to_Have/">Python Nice to Have</a>
                </li>
                <li class="">
                    
    <a class="" href="../Freeze_the_Code/">Freeze the Code</a>
                </li>
                <li class="">
                    
    <a class="" href="../Decorators/">Decorators</a>
                </li>
                <li class="">
                    
    <a class="" href="../Write_Better_Python/">Write Better Python with PEP</a>
                </li>
                <li class="">
                    
    <a class="" href="../Regex/">Regular Expressions (REGEX)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Databases/">Databases</a>
                </li>
                <li class="">
                    
    <a class="" href="../Datetime/">Datetime</a>
                </li>
                <li class="">
                    
    <a class="" href="../Execute_Highlighted_Python_Code_in_gedit/">Execute Highlighted Python Code in gedit</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">SciPy Stack</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Scipy_CS/">Scipy Stack Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../JN_CS/">Jupyter Notebook Cheat Sheets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Scientific Python (the SciPy Stack)/">Scientific Python (the SciPy Stack)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Importing Data into Python/">Importing Data into Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python for Data Science/">Python for Data Science</a>
                </li>
                <li class="">
                    
    <a class="" href="../Tidy_Data_in_Python/">Tidy Data in Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Lists/">Lists</a>
                </li>
                <li class="">
                    
    <a class="" href="../IPython Notebook/">IPython Notebook, Collection</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python Numpy Arrays/">Python Numpy Arrays</a>
                </li>
                <li class="">
                    
    <a class="" href="../Vectors and Arrays (Linear Algebra)/">Vectors and Arrays (Linear Algebra)</a>
                </li>
                <li class="">
                    
    <a class="" href="../Matplotlib, Python Plotting/">Matplotlib, Python Plotting</a>
                </li>
                <li class="">
                    
    <a class="" href="../Viewing+3D+Volumetric+Data+With+Matplotlib/">Viewing 3D Volumetric Data With Matplotlib</a>
                </li>
                <li class="">
                    
    <a class="" href="../Seaborn, Python Statistical Data Visualization Library/">Seaborn, Python's Statistical Data Visualization Library</a>
                </li>
                <li class="">
                    
    <a class="" href="../Pandas+DataFrames/">Pandas DataFrames</a>
                </li>
                <li class="">
                    
    <a class="" href="../Write Idiomatic Pandas Code/">Write Idiomatic Pandas Code</a>
                </li>
                <li class="">
                    
    <a class="" href="../Exploratory Data Analysis/">Exploratory Data Analysis</a>
                </li>
                <li class="">
                    
    <a class="" href="../Intro to data.world in Python/">Intro to data.world in Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python+And+Excel/">Python and Excel</a>
                </li>
                <li class="">
                    
    <a class="" href="../Overview_of_scikit-learn/">Overview of scikit-learn</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">k-NN, Linear regression, Logit, Scaling, Centering, Noise</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#k-nearest-neighbours">k-Nearest Neighbours</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#measure-performance">Measure performance</a></li>
        
            <li><a class="toctree-l4" href="#train-test-split-and-performance-in-practice">Train-test split and performance in practice</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#preprocessing-scaling-and-centering-the-data">Preprocessing: scaling and centering the data</a></li>
    

    <li class="toctree-l3"><a href="#k-nn-scaling-in-practice">k-NN: scaling in practice</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#scale-the-data">Scale the data</a></li>
        
            <li><a class="toctree-l4" href="#run-the-k-nn">Run the k-NN</a></li>
        
            <li><a class="toctree-l4" href="#measure-the-performance">Measure the performance</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#k-nn-recap">k-NN Recap</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#without-scaling">Without scaling</a></li>
        
            <li><a class="toctree-l4" href="#with-scaling">With scaling</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#linear-regression">Linear regression</a></li>
    

    <li class="toctree-l3"><a href="#logistic-regression-logit">Logistic regression (Logit)</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#with-random-numbers">With random numbers</a></li>
        
            <li><a class="toctree-l4" href="#with-the-wine-dataset">With the Wine dataset</a></li>
        
            <li><a class="toctree-l4" href="#scale-the-data_1">Scale the data</a></li>
        
            <li><a class="toctree-l4" href="#run-the-logit-and-measure-the-performance">Run the Logit and measure the performance</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#logit-recap">Logit Recap</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#without-scaling_1">Without scaling</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#noise-and-scaling">Noise and scaling</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#plotting-the-synthesized-data">Plotting the synthesized data</a></li>
        
            <li><a class="toctree-l4" href="#k-nearest-neighbours_1">k-Nearest Neighbours</a></li>
        
            <li><a class="toctree-l4" href="#noise-strength-vs-accuracy-and-the-need-for-scaling">Noise strength vs. accuracy (and the need for scaling)</a></li>
        
            <li><a class="toctree-l4" href="#logit-repeat-the-k-nn-procedure">Logit (Repeat the k-NN procedure)</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../Time_Series_Analysis/">Time Series Analysis</a>
                </li>
                <li class="">
                    
    <a class="" href="../Sentiment_Analysis_with_Twitter/">Sentiment Analysis with Twitter</a>
                </li>
                <li class="">
                    
    <a class="" href="../EDA_Machine_Learning_Feature_Engineering_and_Kaggle/">EDA, Machine Learning, Feature Engineering, and Kaggle</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Courses</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Apprenez a programmer en Python/">Apprenez à programmer en Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Codecademy Python/">Codecademy Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Learn Python the Hard Way/">Learn Python the Hard Way</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python Code Snippets/">Python Code Snippets</a>
                </li>
                <li class="">
                    
    <a class="" href="../Introduction to Python/">Introduction to Python</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Manuals</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Automate the Boring Stuff with Python/">Automate the Boring Stuff with Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Real_Python/">Real Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Managing Your Biological Data with Python/">Managing Your Biological Data with Python</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python for Education/">Python for Education</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ugo_py_doc</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>SciPy Stack &raquo;</li>
        
      
    
    <li>k-NN, Linear regression, Logit, Scaling, Centering, Noise</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <div class="toc"><span class="toctitle">CONTENT</span><ul>
<li><a href="#load-and-explore-the-wine-dataset">Load and explore the Wine dataset</a></li>
<li><a href="#k-nearest-neighbours">k-Nearest Neighbours</a><ul>
<li><a href="#measure-performance">Measure performance</a></li>
<li><a href="#train-test-split-and-performance-in-practice">Train-test split and performance in practice</a></li>
</ul>
</li>
<li><a href="#preprocessing-scaling-and-centering-the-data">Preprocessing: scaling and centering the data</a></li>
<li><a href="#k-nn-scaling-in-practice">k-NN: scaling in practice</a><ul>
<li><a href="#scale-the-data">Scale the data</a></li>
<li><a href="#run-the-k-nn">Run the k-NN</a></li>
<li><a href="#measure-the-performance">Measure the performance</a></li>
</ul>
</li>
<li><a href="#k-nn-recap">k-NN Recap</a><ul>
<li><a href="#without-scaling">Without scaling</a></li>
<li><a href="#with-scaling">With scaling</a></li>
</ul>
</li>
<li><a href="#linear-regression">Linear regression</a></li>
<li><a href="#logistic-regression-logit">Logistic regression (Logit)</a><ul>
<li><a href="#with-random-numbers">With random numbers</a></li>
<li><a href="#with-the-wine-dataset">With the Wine dataset</a></li>
<li><a href="#scale-the-data_1">Scale the data</a></li>
<li><a href="#run-the-logit-and-measure-the-performance">Run the Logit and measure the performance</a></li>
</ul>
</li>
<li><a href="#logit-recap">Logit Recap</a><ul>
<li><a href="#without-scaling_1">Without scaling</a></li>
</ul>
</li>
<li><a href="#noise-and-scaling">Noise and scaling</a><ul>
<li><a href="#plotting-the-synthesized-data">Plotting the synthesized data</a></li>
<li><a href="#k-nearest-neighbours_1">k-Nearest Neighbours</a><ul>
<li><a href="#scale-the-data-run-the-k-nn-and-measure-the-performance">Scale the data, run the k-NN, and measure the performance</a></li>
<li><a href="#add-noise-to-the-signal">Add noise to the signal</a></li>
<li><a href="#run-the-k-nn-and-measure-the-performance">Run the k-NN and measure the performance</a></li>
<li><a href="#scale-the-data-add-noise-run-the-k-nn-and-measure-the-performance">Scale the data, add noise, run the k-NN, and measure the performance</a></li>
</ul>
</li>
<li><a href="#noise-strength-vs-accuracy-and-the-need-for-scaling">Noise strength vs. accuracy (and the need for scaling)</a></li>
<li><a href="#logit-repeat-the-k-nn-procedure">Logit (Repeat the k-NN procedure)</a></li>
</ul>
</li>
</ul>
</div>
<hr />
<p><strong>Foreword</strong></p>
<p>Code snippets and excerpts from the tutorial. Python 3. From DataCamp.</p>
<hr />
<h3 id="load-and-explore-the-wine-dataset">Load and explore the Wine dataset<a class="headerlink" href="#load-and-explore-the-wine-dataset" title="Permanent link">&para;</a></h3>
<p>We use the <a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">wine quality dataset</a> related to red and white vinho verde wine samples, from the north of Portugal.</p>
<pre><code class="python"># import the modules
%pylab inline
import pandas as pd
import matplotlib.pyplot as plt

# set the style
plt.style.use('ggplot')
</code></pre>

<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>
<pre><code class="python"># import the data
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';')
df.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># drop target variable
# only keep the values; the DataFrame becomes a simple array (matrix)
# index (axis=0 / ‘index’) or columns (axis=1 / ‘columns’).
X = df.drop('quality' , axis=1).values

# print the array
print(X)
</code></pre>

<pre><code>[[  7.4     0.7     0.    ...,   3.51    0.56    9.4  ]
 [  7.8     0.88    0.    ...,   3.2     0.68    9.8  ]
 [  7.8     0.76    0.04  ...,   3.26    0.65    9.8  ]
 ..., 
 [  6.3     0.51    0.13  ...,   3.42    0.75   11.   ]
 [  5.9     0.645   0.12  ...,   3.57    0.71   10.2  ]
 [  6.      0.31    0.47  ...,   3.39    0.66   11.   ]]
</code></pre>
<p>The last column is gone from the array. Make it a list instead (or a single-row array).</p>
<pre><code class="python">y1 = df['quality'].values

# print the single-row array
print(y1)
</code></pre>

<pre><code>[5 5 5 ..., 6 5 6]
</code></pre>
<pre><code class="python"># row, col of the DataFrame
df.shape
</code></pre>

<pre><code>(1599, 12)
</code></pre>
<pre><code class="python"># plot all the columns or variables
pd.DataFrame.hist(df, figsize = [15,15]);

plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_8_0.png" /></p>
<p>Notice the range of each variable; some are wider.</p>
<p>Any algorithm, such as k-NN, which cares about the distance between data points. This motivates scaling our data.</p>
<p>Let us turn it into a two-category variable consisting of &lsquo;good&rsquo; (rating &gt; 5) &amp; &lsquo;bad&rsquo; (rating &lt;= 5) qualities.</p>
<pre><code class="python">print(y1)
</code></pre>

<pre><code>[5 5 5 ..., 6 5 6]
</code></pre>
<pre><code class="python"># is the rating &lt;= 5 ?
y = y1 &lt;= 5
print(y)
</code></pre>

<pre><code>[ True  True  True ..., False  True False]
</code></pre>
<p><code>True</code> is worth 1 and <code>False</code> is worth 0.</p>
<pre><code class="python"># plot two histograms
# the original target variable
# and the aggregated target variable
plt.figure(figsize=(20,5));

# left plot
plt.subplot(1, 2, 1 );
plt.hist(y1);
plt.xlabel('original target value')
plt.ylabel('count')

# right plot
plt.subplot(1, 2, 2);
plt.hist(y)
plt.xlabel('aggregated target value')
plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_13_0.png" /></p>
<p>Again, on the right histogram, <code>True</code> = 1 and <code>False</code> = 0.</p>
<h3 id="k-nearest-neighbours">k-Nearest Neighbours<a class="headerlink" href="#k-nearest-neighbours" title="Permanent link">&para;</a></h3>
<h4 id="measure-performance">Measure performance<a class="headerlink" href="#measure-performance" title="Permanent link">&para;</a></h4>
<p><strong>Accuracy</strong> is the default scoring method for both</p>
<ul>
<li>k-Nearest Neighbours and</li>
<li>logistic regression.</li>
</ul>
<p>
<script type="math/tex; mode=display">\text{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}</script>
</p>
<p>Accuracy is commonly defined for binary classification problems in terms of true positives &amp; false negatives. It can also be defined in terms of a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>. </p>
<p><a href="https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/">Other measures</a> of model performance are derived from the confusion matrix: <strong>precision</strong> (true positives divided by the number of true &amp; false positives) and <strong>recall</strong> (number of true positives divided by the number of true positives plus the number of false negatives). </p>
<p>The <strong><a href="https://en.wikipedia.org/wiki/F1_score">F1-score</a></strong> is the harmonic mean of the precision and the recall.</p>
<h4 id="train-test-split-and-performance-in-practice">Train-test split and performance in practice<a class="headerlink" href="#train-test-split-and-performance-in-practice" title="Permanent link">&para;</a></h4>
<p>The rule of thumb is to use approximately </p>
<ul>
<li>80% of the data for training (train set) and</li>
<li>20% for testing (test set).</li>
</ul>
<pre><code class="python">from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.2, 
                                                    random_state=42)
</code></pre>

<pre><code class="python"># the k-NN model
from sklearn import neighbors, linear_model

knn = neighbors.KNeighborsClassifier(n_neighbors = 5)
knn_model_1 = knn.fit(X_train, y_train)
</code></pre>

<pre><code class="python">print('k-NN score for test set: %f' % knn_model_1.score(X_test, y_test))
print('k-NN score for training set: %f' % knn_model_1.score(X_train, y_train))
</code></pre>

<pre><code>k-NN score for test set: 0.612500
k-NN score for training set: 0.774042
</code></pre>
<p>The accuracy, more specifically the test accuracy, is not great.</p>
<p>Let us print out all the <em>other</em> performance measures for the test set.</p>
<pre><code class="python">from sklearn.metrics import classification_report

y_true, y_pred = y_test, knn_model_1.predict(X_test)
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.66      0.64      0.65       179
       True       0.56      0.57      0.57       141

avg / total       0.61      0.61      0.61       320
</code></pre>
<p><em>Other</em> performance measures for the train set.</p>
<pre><code class="python">y_true, y_pred = y_train, knn_model_1.predict(X_train)
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.80      0.76      0.78       676
       True       0.75      0.79      0.77       603

avg / total       0.78      0.77      0.77      1279
</code></pre>
<p>These underperformances might come from the spread in the variables. The range of each variable is different; some are wider.</p>
<h3 id="preprocessing-scaling-and-centering-the-data">Preprocessing: scaling and centering the data<a class="headerlink" href="#preprocessing-scaling-and-centering-the-data" title="Permanent link">&para;</a></h3>
<p>Preprocessing happens before running any model, such as a regression (predicting a continuous variable) or a classification (predicting a discrete variable) using one or another model (k-NN, logistic, decision tree, random forests etc.).</p>
<p>For numerical variables, it is common to either normalize or standardize the data.</p>
<p><strong>Normalization</strong>: <strong>scaling</strong> a dataset so that its minimum is 0 and its maximum 1.</p>
<p>
<script type="math/tex; mode=display">x_{normalized} = \frac{x-x_{min}}{x_{max}-x_{min}}</script>
</p>
<p><strong>Stardardization</strong>: <strong>centering</strong> the data around 0 and to scale with respect to the standard deviation.</p>
<p>
<script type="math/tex; mode=display">x_{standardized} = \frac{x-\mu}{\sigma}</script>
</p>
<p>where <script type="math/tex">\mu</script> and <script type="math/tex">\sigma</script> are the mean and standard deviation of the dataset.</p>
<p>There are other transformatoions, such as the log transformation or the Box-Cox transformation, to make the data look more Gaussian or a normally distributed.</p>
<h3 id="k-nn-scaling-in-practice">k-NN: scaling in practice<a class="headerlink" href="#k-nn-scaling-in-practice" title="Permanent link">&para;</a></h3>
<h4 id="scale-the-data">Scale the data<a class="headerlink" href="#scale-the-data" title="Permanent link">&para;</a></h4>
<pre><code class="python">print(X)
</code></pre>

<pre><code>[[  7.4     0.7     0.    ...,   3.51    0.56    9.4  ]
 [  7.8     0.88    0.    ...,   3.2     0.68    9.8  ]
 [  7.8     0.76    0.04  ...,   3.26    0.65    9.8  ]
 ..., 
 [  6.3     0.51    0.13  ...,   3.42    0.75   11.   ]
 [  5.9     0.645   0.12  ...,   3.57    0.71   10.2  ]
 [  6.      0.31    0.47  ...,   3.39    0.66   11.   ]]
</code></pre>
<pre><code class="python">from sklearn.preprocessing import scale

# minimum is 0 and its maximum 1
Xs = scale(X)
print(Xs)
</code></pre>

<pre><code>[[-0.52835961  0.96187667 -1.39147228 ...,  1.28864292 -0.57920652
  -0.96024611]
 [-0.29854743  1.96744245 -1.39147228 ..., -0.7199333   0.1289504
  -0.58477711]
 [-0.29854743  1.29706527 -1.18607043 ..., -0.33117661 -0.04808883
  -0.58477711]
 ..., 
 [-1.1603431  -0.09955388 -0.72391627 ...,  0.70550789  0.54204194
   0.54162988]
 [-1.39015528  0.65462046 -0.77526673 ...,  1.6773996   0.30598963
  -0.20930812]
 [-1.33270223 -1.21684919  1.02199944 ...,  0.51112954  0.01092425
   0.54162988]]
</code></pre>
<h4 id="run-the-k-nn">Run the k-NN<a class="headerlink" href="#run-the-k-nn" title="Permanent link">&para;</a></h4>
<pre><code class="python">from sklearn.cross_validation import train_test_split

# split
# 80% of the data for training (train set)
# 20% for testing (test set)
Xs_train, Xs_test, y_train, y_test = train_test_split(Xs,
                                                      y,
                                                      test_size=0.2,
                                                      random_state=42)
</code></pre>

<pre><code class="python"># Run
knn_model_2 = knn.fit(Xs_train, y_train)
</code></pre>

<h4 id="measure-the-performance">Measure the performance<a class="headerlink" href="#measure-the-performance" title="Permanent link">&para;</a></h4>
<pre><code class="python">print('k-NN score for test set: %f' % knn_model_2.score(Xs_test, y_test))
print('k-NN score for training set: %f' % knn_model_2.score(Xs_train, y_train))
</code></pre>

<pre><code>k-NN score for test set: 0.712500
k-NN score for training set: 0.814699
</code></pre>
<pre><code class="python">y_true, y_pred = y_test, knn_model_2.predict(Xs_test)

# Test set
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.72      0.79      0.75       179
       True       0.70      0.62      0.65       141

avg / total       0.71      0.71      0.71       320
</code></pre>
<pre><code class="python">y_true, y_pred = y_train, knn_model_2.predict(Xs_train)

# Train set
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.80      0.86      0.83       676
       True       0.83      0.77      0.80       603

avg / total       0.82      0.81      0.81      1279
</code></pre>
<p>Normalization-scaling improves the performance compare to the previous <code>classification_report</code>.</p>
<h3 id="k-nn-recap">k-NN Recap<a class="headerlink" href="#k-nn-recap" title="Permanent link">&para;</a></h3>
<h4 id="without-scaling">Without scaling<a class="headerlink" href="#without-scaling" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Set sc = False 
# Do not scale the features 
sc = False
# Set the number of k in k-NN
nk = 5

# Load data 
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';') 
# Drop target variable 
X = df.drop('quality' , 1).values

# Scale, if desired 
if sc == True: 
  X = scale(X) 

# Target value 
y1 = df['quality'].values # original target variable 
# New target variable: is the rating &lt;= 5?
y = y1 &lt;= 5 

# Split (80/20) the data into a test set and a train set
# X_train, X_test, y_train, y_test 
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42) 

# Train the k-NN model
knn = neighbors.KNeighborsClassifier(n_neighbors = nk)
knn_model = knn.fit(X_train, y_train)

# Print performance on the test set 
print('k-NN accuracy for test set: %f' % knn_model.score(X_test, y_test))
y_true, y_pred = y_test, knn_model.predict(X_test) 
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>k-NN accuracy for test set: 0.612500
             precision    recall  f1-score   support

      False       0.66      0.64      0.65       179
       True       0.56      0.57      0.57       141

avg / total       0.61      0.61      0.61       320
</code></pre>
<h4 id="with-scaling">With scaling<a class="headerlink" href="#with-scaling" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Set sc = True 
# to scale the features 
sc = True
# Set the number of k in k-NN
nk = 5

# Load data 
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';') 
# Drop target variable 
X = df.drop('quality' , 1).values

# Scale, if desired 
if sc == True: 
  X = scale(X) 

# Target value 
y1 = df['quality'].values # original target variable 
# New target variable: is the rating &lt;= 5?
y = y1 &lt;= 5 

# Split (80/20) the data into a test set and a train set
# X_train, X_test, y_train, y_test 
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42) 

# Train the k-NN model
knn = neighbors.KNeighborsClassifier(n_neighbors = nk)
knn_model = knn.fit(X_train, y_train)

# Print performance on the test set 
print('k-NN accuracy for test set: %f' % knn_model.score(X_test, y_test))
y_true, y_pred = y_test, knn_model.predict(X_test) 
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>k-NN accuracy for test set: 0.712500
             precision    recall  f1-score   support

      False       0.72      0.79      0.75       179
       True       0.70      0.62      0.65       141

avg / total       0.71      0.71      0.71       320
</code></pre>
<h3 id="linear-regression">Linear regression<a class="headerlink" href="#linear-regression" title="Permanent link">&para;</a></h3>
<p>Before addressing an alternative to k-NN, the logistic regression or Logit, let us briefly review the linear regresion with a different dataset.</p>
<pre><code class="python"># Import necessary packages
%pylab inline
import pandas as pd
import matplotlib.pyplot as plt

# set the style
plt.style.use('ggplot')

# Import nmore packages
from sklearn import datasets
from sklearn import linear_model
import numpy as np
</code></pre>

<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>
<pre><code class="python"># Load the data
# The data is part of the scikit-learn module
boston = datasets.load_boston()
yb = boston.target.reshape(-1, 1)
Xb = boston['data'][:,5].reshape(-1, 1)

print(yb[:10])
</code></pre>

<pre><code>[[ 24. ]
 [ 21.6]
 [ 34.7]
 [ 33.4]
 [ 36.2]
 [ 28.7]
 [ 22.9]
 [ 27.1]
 [ 16.5]
 [ 18.9]]
</code></pre>
<pre><code class="python">print(Xb[:10])
</code></pre>

<pre><code>[[ 6.575]
 [ 6.421]
 [ 7.185]
 [ 6.998]
 [ 7.147]
 [ 6.43 ]
 [ 6.012]
 [ 6.172]
 [ 5.631]
 [ 6.004]]
</code></pre>
<pre><code class="python"># Plot data
plt.scatter(Xb,yb)
plt.ylabel('value of house /1000 ($)')
plt.xlabel('number of rooms')
</code></pre>

<pre><code>&lt;matplotlib.text.Text at 0x7f3681ae90b8&gt;
</code></pre>
<p><img alt="" src="../img/k-NN_linear_regression/output_40_1.png" /></p>
<pre><code class="python"># Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit( Xb, yb)
</code></pre>

<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre>
<pre><code class="python"># Plot outputs
plt.scatter(Xb, yb,  color='black')
plt.plot(Xb, regr.predict(Xb), color='blue',
         linewidth=3)
plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_42_0.png" /></p>
<h3 id="logistic-regression-logit">Logistic regression (Logit)<a class="headerlink" href="#logistic-regression-logit" title="Permanent link">&para;</a></h3>
<h4 id="with-random-numbers">With random numbers<a class="headerlink" href="#with-random-numbers" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Synthesize data
X1 = np.random.normal(size=150)
y1 = (X1 &gt; 0).astype(np.float)
X1[X1 &gt; 0] *= 4
X1 += .3 * np.random.normal(size=150)
X1 = X1.reshape(-1, 1)
</code></pre>

<pre><code class="python"># Run the classifier
clf = linear_model.LogisticRegression()
clf.fit(X1, y1)
</code></pre>

<pre><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
</code></pre>
<pre><code class="python">X1[:10]
</code></pre>

<pre><code>array([[-0.74466839],
       [ 0.47335714],
       [-1.94951938],
       [ 0.12078443],
       [-1.62121705],
       [-2.23684396],
       [ 7.66984914],
       [-0.31941781],
       [-1.07205326],
       [ 0.85413978]])
</code></pre>
<pre><code class="python"># Order X1
X1_ordered = sorted(X1, reverse=False)

X1_ordered[:10]
</code></pre>

<pre><code>[array([-3.29826361]),
 array([-2.76292445]),
 array([-2.23684396]),
 array([-1.96629089]),
 array([-1.94951938]),
 array([-1.87501025]),
 array([-1.83321548]),
 array([-1.73611093]),
 array([-1.62121705]),
 array([-1.61885181])]
</code></pre>
<pre><code class="python"># Plot the result
plt.scatter(X1.ravel(), y1, color='black', zorder=20 , alpha = 0.5)
plt.plot(X1_ordered, clf.predict_proba(X1_ordered)[:,1], color='blue' , linewidth = 3)
plt.ylabel('target variable')
plt.xlabel('predictor variable')
plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_48_0.png" /></p>
<h4 id="with-the-wine-dataset">With the Wine dataset<a class="headerlink" href="#with-the-wine-dataset" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Load data
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';')

df.head(3)
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Drop target variable
X = df.drop('quality' , 1).values

# Print the array
print(X)
</code></pre>

<pre><code>[[  7.4     0.7     0.    ...,   3.51    0.56    9.4  ]
 [  7.8     0.88    0.    ...,   3.2     0.68    9.8  ]
 [  7.8     0.76    0.04  ...,   3.26    0.65    9.8  ]
 ..., 
 [  6.3     0.51    0.13  ...,   3.42    0.75   11.   ]
 [  5.9     0.645   0.12  ...,   3.57    0.71   10.2  ]
 [  6.      0.31    0.47  ...,   3.39    0.66   11.   ]]
</code></pre>
<p>The last column is gone.</p>
<pre><code class="python">y1 = df['quality'].values

# Print the single-row array
print(y1)
</code></pre>

<pre><code>[5 5 5 ..., 6 5 6]
</code></pre>
<pre><code class="python">df.shape
</code></pre>

<pre><code>(1599, 12)
</code></pre>
<pre><code class="python"># plot the other columns or variables
pd.DataFrame.hist(df, figsize = [15,15]);

plt.show() # facultative in Jypyter
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_55_0.png" /></p>
<p>Let us turn it into a two-category variable consisting of &lsquo;good&rsquo; (rating &gt; 5) &amp; &lsquo;bad&rsquo; (rating &lt;= 5) qualities.</p>
<pre><code class="python"># is the rating &lt;= 5 ?
y = y1 &lt;= 5
print(y)
</code></pre>

<pre><code>[ True  True  True ..., False  True False]
</code></pre>
<pre><code class="python">from sklearn.cross_validation import train_test_split

# split
# 80% of the data for training (train set)
# 20% for testing (test set)
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)
</code></pre>

<pre><code class="python">from sklearn import linear_model

# Initial logistic regression model
lr = linear_model.LogisticRegression()
</code></pre>

<pre><code class="python"># Fit the model
lr = lr.fit(X_train, y_train)
y_true, y_pred = y_train, lr.predict(X_train)

# Evaluate the train set
print('Logistic Regression score for train set: %f' % lr.score(X_train, y_train))
</code></pre>

<pre><code>Logistic Regression score for train set: 0.752932
</code></pre>
<pre><code class="python">print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.77      0.75      0.76       676
       True       0.73      0.75      0.74       603

avg / total       0.75      0.75      0.75      1279
</code></pre>
<pre><code class="python">from sklearn.metrics import classification_report

# Use the test set
y_true, y_pred = y_test, lr.predict(X_test)

# Evaluate the test set
print('Logistic Regression score for test set: %f' % lr.score(X_test, y_test))
</code></pre>

<pre><code>Logistic Regression score for test set: 0.740625
</code></pre>
<pre><code class="python">print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.78      0.74      0.76       179
       True       0.69      0.74      0.71       141

avg / total       0.74      0.74      0.74       320
</code></pre>
<p><strong>Note</strong>: the logistic regression performs better than k-NN without scaling.</p>
<h4 id="scale-the-data_1">Scale the data<a class="headerlink" href="#scale-the-data_1" title="Permanent link">&para;</a></h4>
<pre><code class="python">print(X)
</code></pre>

<pre><code>[[  7.4     0.7     0.    ...,   3.51    0.56    9.4  ]
 [  7.8     0.88    0.    ...,   3.2     0.68    9.8  ]
 [  7.8     0.76    0.04  ...,   3.26    0.65    9.8  ]
 ..., 
 [  6.3     0.51    0.13  ...,   3.42    0.75   11.   ]
 [  5.9     0.645   0.12  ...,   3.57    0.71   10.2  ]
 [  6.      0.31    0.47  ...,   3.39    0.66   11.   ]]
</code></pre>
<pre><code class="python">from sklearn.preprocessing import scale

Xs = scale(X)
print(Xs)
</code></pre>

<pre><code>[[-0.52835961  0.96187667 -1.39147228 ...,  1.28864292 -0.57920652
  -0.96024611]
 [-0.29854743  1.96744245 -1.39147228 ..., -0.7199333   0.1289504
  -0.58477711]
 [-0.29854743  1.29706527 -1.18607043 ..., -0.33117661 -0.04808883
  -0.58477711]
 ..., 
 [-1.1603431  -0.09955388 -0.72391627 ...,  0.70550789  0.54204194
   0.54162988]
 [-1.39015528  0.65462046 -0.77526673 ...,  1.6773996   0.30598963
  -0.20930812]
 [-1.33270223 -1.21684919  1.02199944 ...,  0.51112954  0.01092425
   0.54162988]]
</code></pre>
<h4 id="run-the-logit-and-measure-the-performance">Run the Logit and measure the performance<a class="headerlink" href="#run-the-logit-and-measure-the-performance" title="Permanent link">&para;</a></h4>
<pre><code class="python">from sklearn.cross_validation import train_test_split

# Split 80/20
Xs_train, Xs_test, y_train, y_test = train_test_split(Xs,
                                                      y,
                                                      test_size=0.2,
                                                      random_state=42)
</code></pre>

<pre><code class="python"># Run the logistic regression model
lr_2 = lr.fit(Xs_train, y_train)
</code></pre>

<pre><code class="python"># Fit the model
y_true, y_pred = y_train, lr_2.predict(Xs_train)

# Evaluate the train set
print('Logistic Regression score for train set: %f' % lr_2.score(Xs_train, y_train))
</code></pre>

<pre><code>Logistic Regression score for train set: 0.752150
</code></pre>
<pre><code class="python">print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.77      0.76      0.76       676
       True       0.73      0.75      0.74       603

avg / total       0.75      0.75      0.75      1279
</code></pre>
<pre><code class="python"># Use the test set
y_true, y_pred = y_test, lr_2.predict(Xs_test)

# Evaluate the test set
print('Logistic Regression score for test set: %f' % lr_2.score(Xs_test, y_test))
</code></pre>

<pre><code>Logistic Regression score for test set: 0.740625
</code></pre>
<pre><code class="python">print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       0.79      0.74      0.76       179
       True       0.69      0.74      0.72       141

avg / total       0.74      0.74      0.74       320
</code></pre>
<p>This is very interesting! The performance of logistic regression did not improve with data scaling.</p>
<p>Predictor variables with large ranges that do not effect the target variable, a regression algorithm will make the corresponding coefficients small so that they do not effect predictions so much.</p>
<h3 id="logit-recap">Logit Recap<a class="headerlink" href="#logit-recap" title="Permanent link">&para;</a></h3>
<h4 id="without-scaling_1">Without scaling<a class="headerlink" href="#without-scaling_1" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Set sc = False
# do not scale the features 
sc = False 

# Load the data 
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';') 
X = df.drop('quality' , 1).values # drop target variable 

# Scale, if desired 
if sc == True: 
  X = scale(X) 

# Target value 
y1 = df['quality'].values # original target variable 
y = y1 &lt;= 5  # new target variable: is the rating &lt;= 5? 

# Split (80/20) the data into a test set and a train
# X_train, X_test, y_train, y_test
train_test_split(X, y, test_size=0.2, random_state=42) 

# Train logistic regression model 
lr = linear_model.LogisticRegression() 
lr = lr.fit(X_train, y_train) 

# Print performance on the test set
print('Logistic Regression score for training set: %f' % lr.score(X_train, y_train)) 
y_true, y_pred = y_test, lr.predict(X_test) 
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>Logistic Regression score for training set: 0.752932
             precision    recall  f1-score   support

      False       0.78      0.74      0.76       179
       True       0.69      0.74      0.71       141

avg / total       0.74      0.74      0.74       320
</code></pre>
<h3 id="noise-and-scaling">Noise and scaling<a class="headerlink" href="#noise-and-scaling" title="Permanent link">&para;</a></h3>
<p>The noisier the symthesized data, the more important scaling will be.</p>
<p>Measurements can be in meters and and miles, with small or large ranges. If we scale the data, they end up being the same.</p>
<p>scikit-learn’s <code>make_blobs</code> function to generate 2000 data points that are in 4 clusters (each data point has 2 predictor variables and 1 target variable).</p>
<pre><code class="python">%pylab inline
</code></pre>

<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>
<pre><code class="python"># Generate some clustered data (blobs!)
import numpy as np
from sklearn.datasets.samples_generator import make_blobs

n_samples=2000
X, y = make_blobs(n_samples, centers=4, n_features=2, random_state=0)

print(X)
</code></pre>

<pre><code>[[-0.46530384  1.73299482]
 [-0.33963733  3.84220272]
 [ 2.25309569  0.99541446]
 ..., 
 [ 1.03616476  4.09126428]
 [-0.5901088   3.68821314]
 [ 2.30405277  4.20250584]]
</code></pre>
<pre><code class="python">print(y)
</code></pre>

<pre><code>[2 0 1 ..., 0 2 0]
</code></pre>
<h4 id="plotting-the-synthesized-data">Plotting the synthesized data<a class="headerlink" href="#plotting-the-synthesized-data" title="Permanent link">&para;</a></h4>
<p>Each axis is a predictor variable and the colour is a key to the target variable</p>
<p>All possible target variables are equally represented. In this case (or even if they are approximately equally represented), we say that the class y is balanced.</p>
<pre><code class="python">import matplotlib.pyplot as plt

plt.style.use('ggplot')

plt.figure(figsize=(20,5));
plt.subplot(1, 2, 1 );
plt.scatter(X[:,0] , X[:,1],  c = y, alpha = 0.7);
plt.subplot(1, 2, 2);
plt.hist(y)

plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_81_0.png" /></p>
<p>Plot histograms of the features.</p>
<pre><code class="python">import pandas as pd

# Convert to a DataFrame
df = pd.DataFrame(X)

# Plot it
pd.DataFrame.hist(df, figsize=(20,5))
</code></pre>

<pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f366d3dbba8&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f366d30ca58&gt;]], dtype=object)
</code></pre>
<p><img alt="" src="../img/k-NN_linear_regression/output_83_1.png" /></p>
<p>Split into test &amp; train sets, and plot both sets (train set &gt; test set; 80/20).</p>
<pre><code class="python">from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)
</code></pre>

<pre><code class="python">plt.figure(figsize=(20,5));
plt.subplot(1, 2, 1 );
plt.title('training set')
plt.scatter(X_train[:,0] , X_train[:,1],  c = y_train, alpha = 0.7);
plt.subplot(1, 2, 2);
plt.scatter(X_test[:,0] , X_test[:,1],  c = y_test, alpha = 0.7);
plt.title('test set')

plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_86_0.png" /></p>
<h4 id="k-nearest-neighbours_1">k-Nearest Neighbours<a class="headerlink" href="#k-nearest-neighbours_1" title="Permanent link">&para;</a></h4>
<p>Let’s instantiate a k-Nearest Neighbours classifier and train it on our train set.</p>
<pre><code class="python">from sklearn import neighbors, linear_model

knn = neighbors.KNeighborsClassifier()
knn_model = knn.fit(X_train, y_train)
</code></pre>

<p>Fit the <code>knn_model</code> to the test set and compute the accuracy.</p>
<pre><code class="python">knn_model.score(X_test, y_test)
</code></pre>

<pre><code>0.93500000000000005
</code></pre>
<pre><code class="python">print('k-NN score for test set: %f' % knn_model.score(X_test, y_test))
</code></pre>

<pre><code>k-NN score for test set: 0.935000
</code></pre>
<p>Check out a variety of other metrics.</p>
<pre><code class="python">from sklearn.metrics import classification_report

y_true, y_pred = y_test, knn_model.predict(X_test)
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

          0       0.87      0.90      0.88       106
          1       0.98      0.93      0.95       102
          2       0.90      0.92      0.91       100
          3       1.00      1.00      1.00        92

avg / total       0.94      0.94      0.94       400
</code></pre>
<p>Re-fit <code>knn_model</code> to the train set and compute the accuracy.</p>
<pre><code class="python">print('k-NN score for train set: %f' % knn_model.score(X_train, y_train))
</code></pre>

<pre><code>k-NN score for train set: 0.941875
</code></pre>
<pre><code class="python">from sklearn.metrics import classification_report

y_true, y_pred = y_train, knn_model.predict(X_train)
print(classification_report(y_true, y_pred))
</code></pre>

<pre><code>             precision    recall  f1-score   support

          0       0.88      0.90      0.89       394
          1       0.97      0.96      0.96       398
          2       0.94      0.93      0.93       400
          3       0.99      0.98      0.98       408

avg / total       0.94      0.94      0.94      1600
</code></pre>
<h5 id="scale-the-data-run-the-k-nn-and-measure-the-performance">Scale the data, run the k-NN, and measure the performance<a class="headerlink" href="#scale-the-data-run-the-k-nn-and-measure-the-performance" title="Permanent link">&para;</a></h5>
<pre><code class="python">print(X)
</code></pre>

<pre><code>[[-0.46530384  1.73299482]
 [-0.33963733  3.84220272]
 [ 2.25309569  0.99541446]
 ..., 
 [ 1.03616476  4.09126428]
 [-0.5901088   3.68821314]
 [ 2.30405277  4.20250584]]
</code></pre>
<pre><code class="python">from sklearn.preprocessing import scale

Xs = scale(X)
print(Xs)
</code></pre>

<pre><code>[[-0.26508542 -0.82638395]
 [-0.19594894 -0.0519305 ]
 [ 1.23046484 -1.09720678]
 ..., 
 [ 0.5609601   0.03951927]
 [-0.33374791 -0.10847199]
 [ 1.25849931  0.08036466]]
</code></pre>
<pre><code class="python">from sklearn.cross_validation import train_test_split

Xs_train, Xs_test, y_train, y_test = train_test_split(Xs,
                                                      y,
                                                      test_size=0.2,
                                                      random_state=42)
</code></pre>

<pre><code class="python">plt.figure(figsize=(20,5));

plt.subplot(1, 2, 1 );
plt.scatter(Xs_train[:,0] , Xs_train[:,1],  c = y_train, alpha = 0.7);
plt.title('scaled training set')

plt.subplot(1, 2, 2);
plt.scatter(Xs_test[:,0] , Xs_test[:,1],  c = y_test, alpha = 0.7);
plt.title('scaled test set')

plt.show()
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_101_0.png" /></p>
<pre><code class="python">knn_model_s = knn.fit(Xs_train, y_train)

print('k-NN score for test set: %f' % knn_model_s.score(Xs_test, y_test))
</code></pre>

<pre><code>k-NN score for test set: 0.935000
</code></pre>
<p>It doesn’t perform any better with scaling.</p>
<p>This is most likely because both features were already around the same range.</p>
<h5 id="add-noise-to-the-signal">Add noise to the signal<a class="headerlink" href="#add-noise-to-the-signal" title="Permanent link">&para;</a></h5>
<p>Adding a third variable of Gaussian noise with mean 0 and variable standard deviation <script type="math/tex">\sigma</script>. We call <script type="math/tex">\sigma</script> the strength of the noise and we see that the stronger the noise, the worse the performance of k-Nearest Neighbours.</p>
<pre><code class="python"># Strength of noise term
ns = 10**(3)

# Add noise column to predictor variables
newcol = np.transpose([ns*np.random.randn(n_samples)])
Xn = np.concatenate((X, newcol), axis = 1)

print(Xn)
</code></pre>

<pre><code>[[ -4.65303843e-01   1.73299482e+00  -9.41949646e+01]
 [ -3.39637332e-01   3.84220272e+00  -1.00446506e+03]
 [  2.25309569e+00   9.95414462e-01   2.95697211e+02]
 ..., 
 [  1.03616476e+00   4.09126428e+00  -1.16020635e+02]
 [ -5.90108797e-01   3.68821314e+00   5.60244701e+02]
 [  2.30405277e+00   4.20250584e+00  -8.97600798e+02]]
</code></pre>
<p>Plot the 3D data.</p>
<pre><code class="python">from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(111, projection='3d' , alpha = 0.5)
ax.scatter(Xn[:,0], Xn[:,1], Xn[:,2], c = y)
</code></pre>

<pre><code>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f366d409cf8&gt;
</code></pre>
<p><img alt="" src="../img/k-NN_linear_regression/output_106_1.png" /></p>
<h5 id="run-the-k-nn-and-measure-the-performance">Run the k-NN and measure the performance<a class="headerlink" href="#run-the-k-nn-and-measure-the-performance" title="Permanent link">&para;</a></h5>
<pre><code class="python"># Split into train-test sets
Xn_train, Xn_test, y_train, y_test = train_test_split(Xn,
                                                      y, 
                                                      test_size=0.2, 
                                                      random_state=42)
</code></pre>

<pre><code class="python"># Run the model
knn = neighbors.KNeighborsClassifier()
knn_model = knn.fit(Xn_train, y_train)
</code></pre>

<pre><code class="python"># Evaluate
print('k-NN score for test set: %f' % knn_model.score(Xn_test, y_test))
</code></pre>

<pre><code>k-NN score for test set: 0.337500
</code></pre>
<p>Horrible!</p>
<h5 id="scale-the-data-add-noise-run-the-k-nn-and-measure-the-performance">Scale the data, add noise, run the k-NN, and measure the performance<a class="headerlink" href="#scale-the-data-add-noise-run-the-k-nn-and-measure-the-performance" title="Permanent link">&para;</a></h5>
<pre><code class="python"># Scale
Xns = scale(Xn)

print(Xns)
</code></pre>

<pre><code>[[-0.26508542 -0.82638395 -0.07164275]
 [-0.19594894 -0.0519305  -0.98584539]
 [ 1.23046484 -1.09720678  0.31993383]
 ..., 
 [ 0.5609601   0.03951927 -0.09356271]
 [-0.33374791 -0.10847199  0.58562421]
 [ 1.25849931  0.08036466 -0.87851945]]
</code></pre>
<pre><code class="python"># Apply noise
s = int(.2*n_samples)
Xns_train = Xns[s:]
y_train = y[s:]
Xns_test = Xns[:s]
y_test = y[:s]

# Run the model
knn = neighbors.KNeighborsClassifier()
knn_models = knn.fit(Xns_train, y_train)

# Evaluate
print('k-NN score for test set: %f' % knn_models.score(Xns_test, y_test))
</code></pre>

<pre><code>k-NN score for test set: 0.917500
</code></pre>
<p>After scaling the data, the model performs nearly as well as were there no noise introduced.</p>
<h4 id="noise-strength-vs-accuracy-and-the-need-for-scaling">Noise strength vs. accuracy (and the need for scaling)<a class="headerlink" href="#noise-strength-vs-accuracy-and-the-need-for-scaling" title="Permanent link">&para;</a></h4>
<p>How the noise strength can effect model accuracy?</p>
<p>Create a function to split the data and run the model. </p>
<p>Use the function in a loop.</p>
<pre><code class="python">def accu( X, y):
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        test_size=0.2,
                                                        random_state=42)

    knn = neighbors.KNeighborsClassifier()
    knn_model = knn.fit(X_train, y_train)

    return(knn_model.score(X_test, y_test))
</code></pre>

<pre><code class="python"># Set the variables
noise = [10**i for i in np.arange(0,6)]
A1 = np.zeros(len(noise))
A2 = np.zeros(len(noise))
count = 0
</code></pre>

<pre><code class="python">print(noise)
</code></pre>

<pre><code>[1, 10, 100, 1000, 10000, 100000]
</code></pre>
<pre><code class="python">print(A1)
print(A2)
</code></pre>

<pre><code>[ 0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.]
</code></pre>
<pre><code class="python"># Run the loop
for ns in noise:
    newcol = np.transpose([ns*np.random.randn(n_samples)])
    Xn = np.concatenate((X, newcol), axis = 1)
    Xns = scale(Xn)
    A1[count] = accu( Xn, y)
    A2[count] = accu( Xns, y)
    count += 1
</code></pre>

<pre><code class="python"># Plot the results
plt.scatter( noise, A1 )
plt.plot( noise, A1, label = 'unscaled', linewidth = 2)
plt.scatter( noise, A2 , c = 'r')
plt.plot( noise, A2 , label = 'scaled', linewidth = 2)
plt.xscale('log')
plt.xlabel('Noise strength')
plt.ylabel('Accuracy')
plt.legend(loc=3);
</code></pre>

<p><img alt="" src="../img/k-NN_linear_regression/output_120_0.png" /></p>
<pre><code class="python">print(A1)
print(A2)
</code></pre>

<pre><code>[ 0.9225  0.9175  0.8025  0.3275  0.22    0.2525]
[ 0.91    0.9175  0.9325  0.9075  0.9325  0.92  ]
</code></pre>
<p>The more noise there is in the nuisance variable, the more important it is to scale the data for the k-NN model.</p>
<blockquote>
<p>More noise, more scaling.</p>
</blockquote>
<h4 id="logit-repeat-the-k-nn-procedure">Logit (Repeat the k-NN procedure)<a class="headerlink" href="#logit-repeat-the-k-nn-procedure" title="Permanent link">&para;</a></h4>
<pre><code class="python"># Change the exponent of 10 to alter the amount of noise
ns = 10**(3) # Strength of noise term

# Set sc = True if we want to scale the features
sc = True
</code></pre>

<pre><code class="python"># Import packages
import numpy as np
from sklearn.cross_validation import train_test_split
from sklearn import neighbors, linear_model
from sklearn.preprocessing import scale
from sklearn.datasets.samples_generator import make_blobs
</code></pre>

<pre><code class="python"># Generate some data
n_samples=2000
X, y = make_blobs(n_samples, 
                  centers=4, 
                  n_features=2,
                  random_state=0)
</code></pre>

<pre><code class="python"># Add noise column to predictor variables
newcol = np.transpose([ns*np.random.randn(n_samples)])
Xn = np.concatenate((X, newcol), axis = 1)
</code></pre>

<pre><code class="python"># Scale if desired
if sc == True:
    Xn = scale(Xn)
</code></pre>

<pre><code class="python"># Train model and test after splitting
Xn_train, Xn_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=42)
lr = linear_model.LogisticRegression()
lr_model = lr.fit(Xn_train, y_train)
print('logistic regression score for test set: %f' % lr_model.score(Xn_test, y_test))
</code></pre>

<pre><code>logistic regression score for test set: 0.942500
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Time_Series_Analysis/" class="btn btn-neutral float-right" title="Time Series Analysis">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Overview_of_scikit-learn/" class="btn btn-neutral" title="Overview of scikit-learn"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>© Ugo Sparks</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Overview_of_scikit-learn/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Time_Series_Analysis/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../mathjaxhelper.js"></script>

</body>
</html>
